{
  "name": "Sequence to Sequence Learning",
  "timestamp": "2025-07-08T08:47:40.890Z",
  "responses": [
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n**신경망을 이용한 시퀀스-투-시퀀스 학습**\n\n## 단어 및 의미 리스트:\n\n*   **Sequence:** 시퀀스, 연속 (된 것), 순서, 차례\n*   **to:** ~로, ~에 (방향, 도착점, 목적지 등을 나타냄)\n*   **Learning:** 학습, 배움\n*   **with:** ~을 가지고, ~와 함께, ~을 이용하여\n*   **Neural:** 신경의, 신경망의\n*   **Networks:** 네트워크, 망, 연결망\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: Sequence to Sequence Learning\n### 번역: 시퀀스-투-시퀀스 학습\n---\n\n\n### 원문: with Neural Networks\n### 번역: 신경망을 이용한\n---\n\n\n### 원문: Sequence to Sequence Learning with Neural Networks\n### 번역: 신경망을 이용한 시퀀스-투-시퀀스 학습\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n심층 신경망(DNN)은 어려운 학습 과제에서 뛰어난 성능을 달성한 강력한 모델입니다. DNN은 대규모 레이블이 지정된 훈련 데이터 세트를 사용할 수 있을 때마다 잘 작동하지만, 시퀀스를 시퀀스로 매핑하는 데는 사용할 수 없습니다. 본 논문에서는 시퀀스 구조에 대한 최소한의 가정만 하는 일반적인 엔드투엔드 시퀀스 학습 접근 방식을 제시합니다. 우리의 방법은 다층 장단기 기억(LSTM)을 사용하여 입력 시퀀스를 고정된 차원의 벡터로 매핑한 다음, 다른 심층 LSTM을 사용하여 해당 벡터에서 대상 시퀀스를 디코딩합니다. 우리의 주요 결과는 WMT’14 데이터 세트의 영어-프랑스어 번역 작업에서 LSTM으로 생성된 번역이 전체 테스트 세트에서 34.8의 BLEU 점수를 달성했다는 것입니다. 여기서 LSTM의 BLEU 점수는 어휘에 없는 단어에 대해 페널티를 받았습니다. 또한 LSTM은 긴 문장에서 어려움을 겪지 않았습니다. 비교를 위해 구문 기반 SMT 시스템은 동일한 데이터 세트에서 33.3의 BLEU 점수를 달성합니다. 우리가 LSTM을 사용하여 위에서 언급한 SMT 시스템에서 생성된 1000개의 가설을 재정렬했을 때, BLEU 점수는 36.5로 증가했으며, 이는 이 작업에서 이전의 최고 결과에 가깝습니다. LSTM은 또한 단어 순서에 민감하고 능동태 및 수동태에 상대적으로 불변하는 합리적인 구문 및 문장 표현을 학습했습니다. 마지막으로, 우리는 모든 원본 문장에서 단어 순서를 뒤집는 것(대상 문장은 제외)이 LSTM의 성능을 현저히 향상시키는 것을 발견했습니다. 왜냐하면 그렇게 함으로써 원본 및 대상 문장 사이에 많은 단기 의존성을 도입하여 최적화 문제를 더 쉽게 만들었기 때문입니다.\n\n## 단어 및 의미 리스트\n\n*   **Deep Neural Networks (DNNs):** 심층 신경망\n*   **models:** 모델\n*   **achieved:** 달성했다\n*   **excellent:** 뛰어난, 훌륭한\n*   **performance:** 성능\n*   **difficult:** 어려운\n*   **learning:** 학습\n*   **tasks:** 과제, 작업\n*   **Although:** ~이지만, 비록 ~이지만\n*   **work well:** 잘 작동하다\n*   **whenever:** ~할 때마다\n*   **large:** 큰, 대규모의\n*   **labeled:** 레이블이 지정된\n*   **training sets:** 훈련 데이터 세트\n*   **available:** 사용 가능한\n*   **cannot:** ~할 수 없다\n*   **used:** 사용된\n*   **map:** 매핑하다\n*   **sequences:** 시퀀스\n*   **paper:** 논문\n*   **present:** 제시하다, 발표하다\n*   **general:** 일반적인\n*   **end-to-end:** 엔드투엔드 (처음부터 끝까지)\n*   **approach:** 접근 방식\n*   **makes:** 만들다\n*   **minimal:** 최소한의\n*   **assumptions:** 가정\n*   **structure:** 구조\n*   **method:** 방법\n*   **multilayered:** 다층의\n*   **Long Short-TermMemory (LSTM):** 장단기 기억\n*   **input:** 입력\n*   **vector:** 벡터\n*   **fixed:** 고정된\n*   **dimensionality:** 차원\n*   **decode:** 디코딩하다, 해독하다\n*   **target:** 대상\n*   **main result:** 주요 결과\n*   **English:** 영어\n*   **French:** 프랑스어\n*   **translation:** 번역\n*   **task:** 작업, 과제\n*   **dataset:** 데이터 세트\n*   **translations:** 번역문\n*   **produced:** 생성된\n*   **achieve:** 달성하다\n*   **BLEU score:** BLEU 점수 (번역 품질 평가 지표)\n*   **entire:** 전체의\n*   **test set:** 테스트 세트\n*   **penalized:** 페널티를 받았다\n*   **out-of-vocabulary:** 어휘에 없는\n*   **words:** 단어\n*   **Additionally:** 추가적으로, 또한\n*   **difficulty:** 어려움\n*   **long sentences:** 긴 문장\n*   **comparison:** 비교\n*   **phrase-based:** 구문 기반의\n*   **SMT system:** SMT 시스템 (통계 기계 번역 시스템)\n*   **hypotheses:** 가설\n*   **aforementioned:** 위에서 언급한\n*   **rerank:** 재정렬하다\n*   **increases:** 증가하다\n*   **close to:** ~에 가깝다\n*   **previous:** 이전의\n*   **best result:** 최고의 결과\n*   **learned:** 학습했다\n*   **sensible:** 합리적인\n*   **phrase:** 구문\n*   **sentence:** 문장\n*   **representations:** 표현\n*   **sensitive:** 민감한\n*   **word order:** 단어 순서\n*   **relatively:** 상대적으로\n*   **invariant:** 불변의\n*   **active voice:** 능동태\n*   **passive voice:** 수동태\n*   **Finally:** 마지막으로\n*   **found:** 발견했다\n*   **reversing:** 뒤집는 것\n*   **order:** 순서\n*   **source sentences:** 원본 문장\n*   **improved:** 향상시켰다\n*   **performance:** 성능\n*   **markedly:** 현저히\n*   **because:** 왜냐하면\n*   **doing so:** 그렇게 함으로써\n*   **introduced:** 도입했다\n*   **short term dependencies:** 단기 의존성\n*   **between:** ~ 사이에\n*   **optimization problem:** 최적화 문제\n*   **easier:** 더 쉬운\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks.\n### 번역: 심층 신경망(DNN)은 어려운 학습 과제에서 뛰어난 성능을 달성한 강력한 모델입니다.\n---\n\n\n### 원문: Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences.\n### 번역: DNN은 레이블이 지정된 대규모 훈련 데이터 세트를 사용할 수 있을 때는 잘 작동하지만, 시퀀스를 시퀀스로 매핑하는 데는 사용할 수 없습니다.\n---\n\n\n### 원문: In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure.\n### 번역: 본 논문에서는 시퀀스 구조에 대한 가정을 최소화하는 시퀀스 학습에 대한 일반적인 엔드 투 엔드 접근 방식을 제시합니다.\n---\n\n\n### 원문: Our method\nuses a multilayered Long Short-TermMemory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector.\n### 번역: 우리 방법은 다층 LSTM(Long Short-Term Memory)을 사용하여 입력 시퀀스를 고정된 차원의 벡터로 매핑한 다음, 다른 심층 LSTM을 사용하여 해당 벡터에서 대상 시퀀스를 디코딩합니다.\n---\n\n\n### 원문: Our main result is that on an English to French\ntranslation task fromtheWMT’14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU\nscore was penalized on out-of-vocabulary words.\n### 번역: 우리의 주요 결과는 WMT'14 데이터 세트의 영어-프랑스어 번역 작업에서 LSTM에 의해 생성된 번역이 전체 테스트 세트에서 BLEU 점수 34.8점을 달성했다는 것입니다. 이때 LSTM의 BLEU 점수는 어휘에 없는 단어에 대해 감점을 받았습니다.\n---\n\n\n### 원문: Additionally, the LSTM did not\nhave difficulty on long sentences.\n### 번역: 또한 LSTM은 긴 문장에서도 어려움을 겪지 않았습니다.\n---\n\n\n### 원문: For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset.\n### 번역: 비교를 위해, 구문 기반 SMT 시스템은 동일한 데이터 세트에서 BLEU 점수 33.3점을 달성합니다.\n---\n\n\n### 원문: When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask.\n### 번역: 언급된 SMT 시스템에서 생성된 1000개의 가설을 LSTM을 사용하여 재정렬했을 때, 해당 BLEU 점수는 36.5점으로 증가했으며, 이는 이 작업에서 이전 최고 결과에 근접합니다.\n---\n\n\n### 원문: The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the passive\nvoice.\n### 번역: 또한 LSTM은 단어 순서에 민감하고 능동태 및 수동태에 상대적으로 불변하는 합리적인 구문 및 문장 표현을 학습했습니다.\n---\n\n\n### 원문: Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM’s performancemarkedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.\n### 번역: 마지막으로, 모든 소스 문장의 단어 순서를 뒤집는 것(대상 문장은 제외)이 LSTM의 성능을 현저히 향상시키는 것을 발견했습니다. 이는 소스 문장과 대상 문장 사이에 많은 단기 종속성을 도입하여 최적화 문제를 더 쉽게 만들었기 때문입니다.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n**1. 서론**\n\n심층 신경망(DNN)은 매우 강력한 머신 러닝 모델로서 음성 인식[13, 7] 및 시각 객체 인식[19, 6, 21, 20]과 같은 어려운 문제에서 뛰어난 성능을 달성합니다. DNN은 적당한 단계 수로 임의의 병렬 연산을 수행할 수 있기 때문에 강력합니다. DNN의 강력함에 대한 놀라운 예시는 2차 크기의 은닉층을 2개만 사용하여 N 비트 숫자를 정렬할 수 있다는 것입니다[27]. 따라서, 신경망은 기존의 통계 모델과 관련이 있지만 복잡한 계산을 학습합니다. 더욱이, 레이블이 지정된 훈련 세트에 네트워크의 매개변수를 지정하기에 충분한 정보가 있는 경우, 큰 DNN은 지도 역전파를 통해 훈련될 수 있습니다. 따라서, 큰 DNN의 매개변수 설정으로 좋은 결과를 얻을 수 있다면 (예를 들어, 인간이 작업을 매우 빠르게 해결할 수 있기 때문에), 지도 역전파는 이러한 매개변수를 찾아 문제를 해결할 것입니다.\n\n## 단어 및 의미 리스트\n\n다음은 원문에서 추출된 단어들과 그에 대한 한글 의미입니다.\n\n*   **Introduction:** 서론\n*   **Deep Neural Networks (DNNs):** 심층 신경망\n*   **extremely:** 매우, 극도로\n*   **powerful:** 강력한\n*   **machine learning:** 머신 러닝, 기계 학습\n*   **models:** 모델, 모형\n*   **achieve:** 달성하다, 이루다\n*   **excellent:** 뛰어난, 훌륭한\n*   **performance:** 성능\n*   **difficult:** 어려운\n*   **problems:** 문제\n*   **speech recognition:** 음성 인식\n*   **visual object recognition:** 시각 객체 인식\n*   **arbitrary:** 임의의\n*   **parallel:** 병렬의\n*   **computation:** 연산, 계산\n*   **modest:** 적당한, 알맞은\n*   **steps:** 단계\n*   **surprising:** 놀라운\n*   **example:** 예시, 사례\n*   **ability:** 능력\n*   **sort:** 정렬하다\n*   **numbers:** 숫자\n*   **hidden layers:** 은닉층\n*   **quadratic:** 2차의, 제곱의\n*   **size:** 크기\n*   **related:** 관련된\n*   **conventional:** 기존의, 전통적인\n*   **statistical models:** 통계 모델\n*   **learn:** 배우다, 학습하다\n*   **intricate:** 복잡한, 난해한\n*   **furthermore:** 더욱이, 게다가\n*   **large:** 큰, 거대한\n*   **trained:** 훈련된, 학습된\n*   **supervised backpropagation:** 지도 역전파\n*   **labeled training set:** 레이블이 지정된 훈련 세트\n*   **enough:** 충분한\n*   **information:** 정보\n*   **specify:** 지정하다, 명시하다\n*   **network's parameters:** 네트워크의 매개변수\n*   **parameter setting:** 매개변수 설정\n*   **good results:** 좋은 결과\n*   **solve:** 해결하다\n*   **task:** 작업, 과제\n*   **rapidly:** 빠르게, 신속하게\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n1. 서론\n심층 신경망(DNN)은 매우 강력한 머신 러닝 모델로서, 음성 인식 [13, 7] 및 시각적 객체 인식 [19, 6, 21, 20]과 같은 어려운 문제에서 뛰어난 성능을 달성합니다. DNN은 적당한 단계 수만으로 임의의 병렬 연산을 수행할 수 있기 때문에 강력합니다. DNN의 강력함에 대한 놀라운 예는 2차 크기의 숨겨진 레이어 2개만을 사용하여 N 비트 숫자를 정렬할 수 있다는 것입니다 [27]. 따라서, 신경망은 기존의 통계 모델과 관련이 있지만, 복잡한 계산을 학습합니다. 더욱이, 레이블이 지정된 훈련 세트가 네트워크의 매개변수를 지정할 만큼 충분한 정보를 가지고 있다면, 대규모 DNN은 지도 역전파를 통해 훈련될 수 있습니다. 따라서, 대규모 DNN의 매개변수 설정이 좋은 결과를 달성할 수 있다면 (예를 들어, 사람이 작업을 매우 빠르게 해결할 수 있기 때문에), 지도 역전파는 이러한 매개변수를 찾아 문제를 해결할 것입니다.\n\n## 원문 단어 및 한글 의미 리스트:\n\n*   **Introduction:** 서론\n*   **Deep Neural Networks (DNNs):** 심층 신경망 (DNN)\n*   **extremely:** 매우, 극도로\n*   **powerful:** 강력한\n*   **machine learning:** 머신 러닝, 기계 학습\n*   **models:** 모델\n*   **achieve:** 달성하다, 이루다\n*   **excellent:** 뛰어난, 훌륭한\n*   **performance:** 성능\n*   **difficult:** 어려운\n*   **problems:** 문제\n*   **speech recognition:** 음성 인식\n*   **visual object recognition:** 시각적 객체 인식\n*   **arbitrary:** 임의의, 제멋대로의\n*   **parallel:** 병렬의\n*   **computation:** 연산, 계산\n*   **modest:** 적당한, 알맞은\n*   **steps:** 단계\n*   **surprising:** 놀라운\n*   **example:** 예\n*   **ability:** 능력\n*   **sort:** 정렬하다\n*   **hidden layers:** 숨겨진 레이어, 은닉층\n*   **quadratic:** 2차의, 제곱의\n*   **size:** 크기\n*   **related:** 관련된\n*   **conventional:** 기존의, 전통적인\n*   **statistical models:** 통계 모델\n*   **learn:** 학습하다, 배우다\n*   **intricate:** 복잡한, 난해한\n*   **furthermore:** 더욱이, 게다가\n*   **large:** 큰, 대규모의\n*   **trained:** 훈련된\n*   **supervised backpropagation:** 지도 역전파\n*   **labeled:** 레이블이 지정된, 라벨링된\n*   **training set:** 훈련 세트\n*   **enough:** 충분한\n*   **information:** 정보\n*   **specify:** 지정하다, 명시하다\n*   **network's parameters:** 네트워크의 매개변수\n*   **Thus:** 따라서, 그러므로\n*   **parameter setting:** 매개변수 설정\n*   **good results:** 좋은 결과\n*   **example:** 예\n*   **humans:** 인간\n*   **solve:** 해결하다\n*   **task:** 작업, 과제\n*   **rapidly:** 빠르게, 급속히\n*   **find:** 찾다\n*   **solve:** 해결하다\n*   **problem:** 문제\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 1 Introduction\n### 번역: 1. 서론\n\n---\n\n\n### 원문: Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent\nperformance on difficult problems such as speech recognition [13, 7] and visual object recognition\n[19, 6, 21, 20].\n### 번역: 심층 신경망(DNN)은 매우 강력한 머신러닝 모델로, 음성 인식[13, 7] 및 시각 객체 인식[19, 6, 21, 20]과 같은 어려운 문제에서 뛰어난 성능을 보여줍니다.\n\n---\n\n\n### 원문: DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps.\n### 번역: DNN이 강력한 이유는 적은 단계만으로도 임의의 병렬 연산을 수행할 수 있기 때문입니다.\n\n---\n\n\n### 원문: A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27].\n### 번역: DNN의 놀라운 능력 중 하나는 이차 크기의 은닉층 2개만을 사용하여 N개의 N비트 숫자를 정렬할 수 있다는 것입니다[27].\n\n---\n\n\n### 원문: So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation.\n### 번역: 따라서 신경망은 기존 통계 모델과 관련이 있지만, 복잡한 계산을 학습합니다.\n\n---\n\n\n### 원문: Furthermore, large\nDNNs can be trained with supervised backpropagationwhenever the labeled training set has enough\ninformation to specify the network’s parameters.\n### 번역: 또한, 레이블이 지정된 훈련 세트가 네트워크의 매개변수를 지정할 수 있을 만큼 충분한 정보를 가지고 있다면, 지도 학습 역전파를 통해 대규모 DNN을 훈련시킬 수 있습니다.\n\n---\n\n\n### 원문: Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem.\n### 번역: 따라서, 대규모 DNN의 매개변수 설정 중 좋은 결과를 얻을 수 있는 설정이 존재한다면 (예를 들어, 인간이 그 작업을 매우 빠르게 해결할 수 있기 때문에), 지도 학습 역전파는 이러한 매개변수를 찾아 문제를 해결할 것입니다.\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 1 Introduction\n### 번역: 1. 서론\n---\n\n\n### 원문: Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent\nperformance on difficult problems such as speech recognition [13, 7] and visual object recognition\n[19, 6, 21, 20].\n### 번역: 심층 신경망(DNN)은 매우 강력한 머신러닝 모델로, 음성 인식[13, 7]과 시각 객체 인식[19, 6, 21, 20]과 같은 어려운 문제에서 뛰어난 성능을 보인다.\n---\n\n\n### 원문: DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps.\n### 번역: DNN이 강력한 이유는 적은 단계만으로 임의의 병렬 계산을 수행할 수 있기 때문이다.\n---\n\n\n### 원문: A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27].\n### 번역: DNN의 능력에 대한 놀라운 예시는 2차원 크기의 은닉층 2개만 사용하여 N비트 숫자 N개를 정렬할 수 있다는 점이다[27].\n---\n\n\n### 원문: So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation.\n### 번역: 따라서, 신경망은 기존 통계 모델과 관련이 있지만, 복잡한 계산을 학습한다.\n---\n\n\n### 원문: Furthermore, large\nDNNs can be trained with supervised backpropagationwhenever the labeled training set has enough\ninformation to specify the network’s parameters.\n### 번역: 더욱이, 레이블이 지정된 훈련 데이터 세트가 네트워크의 매개변수를 지정하기에 충분한 정보를 가지고 있다면, 대규모 DNN은 지도 역전파를 통해 훈련될 수 있다.\n---\n\n\n### 원문: Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem.\n### 번역: 따라서, 좋은 결과를 얻는 대규모 DNN의 매개변수 설정이 존재한다면 (예를 들어, 인간이 작업을 매우 빠르게 해결할 수 있기 때문에), 지도 역전파는 이러한 매개변수를 찾아내어 문제를 해결할 것이다.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\nDNN(심층 신경망)은 유연성과 강력한 성능에도 불구하고, 입력과 목표를 고정된 차원의 벡터로 합리적으로 인코딩할 수 있는 문제에만 적용될 수 있습니다. 이는 중요한 제약인데, 많은 중요한 문제들이 길이가 사전에 알려지지 않은 시퀀스로 가장 잘 표현되기 때문입니다. 예를 들어, 음성 인식과 기계 번역은 순차적인 문제입니다. 마찬가지로, 질의 응답 또한 질문을 나타내는 단어 시퀀스를 답변을 나타내는 단어 시퀀스로 매핑하는 것으로 볼 수 있습니다. 따라서 시퀀스를 시퀀스로 매핑하는 것을 학습하는 도메인 독립적인 방법이 유용할 것이라는 점은 분명합니다.\n\n## 원문 단어 및 한글 의미 리스트:\n\n*   **Despite:** ~에도 불구하고\n*   **their:** 그들의, 그것들의\n*   **flexibility:** 유연성\n*   **and:** 그리고\n*   **power:** 힘, 능력, 성능\n*   **DNNs:** DNN(심층 신경망)\n*   **can:** ~할 수 있다\n*   **only:** 오직, 단지\n*   **be:** ~이다\n*   **applied:** 적용되다\n*   **to:** ~에, ~에게\n*   **problems:** 문제들\n*   **whose:** 누구의, 어느 것의\n*   **inputs:** 입력\n*   **targets:** 목표\n*   **can:** ~할 수 있다\n*   **be:** ~이다\n*   **sensibly:** 합리적으로, 분별 있게\n*   **encoded:** 인코딩되다, 부호화되다\n*   **with:** ~와 함께, ~으로\n*   **vectors:** 벡터\n*   **of:** ~의\n*   **fixed:** 고정된\n*   **dimensionality:** 차원\n*   **It:** 그것은\n*   **is:** ~이다\n*   **a:** 하나의\n*   **significant:** 중요한, 상당한\n*   **limitation:** 제약, 제한\n*   **since:** ~때문에, ~이므로\n*   **many:** 많은\n*   **important:** 중요한\n*   **problems:** 문제들\n*   **are:** ~이다\n*   **best:** 가장 좋은, 최고\n*   **expressed:** 표현되다\n*   **with:** ~와 함께, ~으로\n*   **sequences:** 시퀀스, 연속\n*   **whose:** 누구의, 어느 것의\n*   **lengths:** 길이\n*   **are:** ~이다\n*   **not:** 아니다\n*   **known:** 알려진\n*   **a-priori:** 사전에, 선험적으로\n*   **For:** ~에 대해, ~을 위해\n*   **example:** 예를 들어\n*   **speech:** 음성, 말\n*   **recognition:** 인식\n*   **and:** 그리고\n*   **machine:** 기계\n*   **translation:** 번역\n*   **are:** ~이다\n*   **sequential:** 순차적인, 연속적인\n*   **problems:** 문제들\n*   **Likewise:** 마찬가지로\n*   **question:** 질문\n*   **answering:** 응답, 답변\n*   **can:** ~할 수 있다\n*   **also:** 또한\n*   **be:** ~이다\n*   **seen:** 보여지다, 간주되다\n*   **as:** ~로서, ~처럼\n*   **mapping:** 매핑, 대응\n*   **a:** 하나의\n*   **sequence:** 시퀀스, 연속\n*   **of:** ~의\n*   **words:** 단어\n*   **representing:** 나타내는, 대표하는\n*   **the:** 그\n*   **question:** 질문\n*   **to:** ~에, ~에게\n*   **a:** 하나의\n*   **sequence:** 시퀀스, 연속\n*   **of:** ~의\n*   **words:** 단어\n*   **representing:** 나타내는, 대표하는\n*   **the:** 그\n*   **answer:** 답변\n*   **It:** 그것은\n*   **is:** ~이다\n*   **therefore:** 따라서\n*   **clear:** 분명한\n*   **that:** ~라는 것\n*   **a:** 하나의\n*   **domain-independent:** 도메인 독립적인\n*   **method:** 방법\n*   **that:** ~하는\n*   **learns:** 학습하다\n*   **to:** ~하기 위해\n*   **map:** 매핑하다, 대응시키다\n*   **sequences:** 시퀀스, 연속\n*   **to:** ~에, ~에게\n*   **sequences:** 시퀀스, 연속\n*   **would:** ~할 것이다\n*   **be:** ~이다\n*   **useful:** 유용한"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of fixed dimensionality.\n### 번역: 유연성과 강력한 성능에도 불구하고, 심층 신경망(DNN)은 입력과 출력이 고정된 차원의 벡터로 적절하게 인코딩될 수 있는 문제에만 적용할 수 있습니다.\n---\n\n\n### 원문: It is a significant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\n### 번역: 이는 중요한 제약인데, 왜냐하면 많은 중요한 문제들은 길이가 미리 알려지지 않은 시퀀스로 표현하는 것이 가장 좋기 때문입니다.\n---\n\n\n### 원문: For example, speech recognition and machine translation are sequential problems.\n### 번역: 예를 들어, 음성 인식과 기계 번역은 순차적인 문제입니다.\n---\n\n\n### 원문: Likewise, question\nanswering can also be seen as mapping a sequence of words representing the question to a\n1\nsequence of words representing the answer.\n### 번역: 마찬가지로, 질의 응답 또한 질문을 나타내는 단어 시퀀스를 답변을 나타내는 단어 시퀀스로 매핑하는 것으로 볼 수 있습니다.\n---\n\n\n### 원문: It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\n### 번역: 따라서 시퀀스를 다른 시퀀스로 매핑하는 방법을 학습하는, 특정 분야에 구애받지 않는 방법이 유용할 것이라는 점은 분명합니다.\n---\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n시퀀스는 DNN(심층 신경망)에게 입출력의 차원이 알려져 있고 고정되어 있어야 한다는 제약을 주므로 어려움을 제기합니다. 본 논문에서는 LSTM(Long Short-Term Memory) 아키텍처 [16]의 간단한 적용이 일반적인 시퀀스-투-시퀀스 문제를 해결할 수 있음을 보여줍니다. 핵심 아이디어는 하나의 LSTM을 사용하여 입력 시퀀스를 한 번에 한 타임스텝씩 읽어 들여 큰 고정 차원의 벡터 표현을 얻고, 다른 LSTM을 사용하여 해당 벡터에서 출력 시퀀스를 추출하는 것입니다(그림 1). 두 번째 LSTM은 기본적으로 순환 신경망 언어 모델 [28, 23, 30]이지만, 입력 시퀀스에 조건화되어 있다는 점이 다릅니다. LSTM의 장거리 시간 의존성이 있는 데이터에 대해 성공적으로 학습하는 능력은 입력과 해당 출력 간에 상당한 시간차가 존재하므로 이 애플리케이션에 대한 자연스러운 선택이 됩니다(그림 1).\n\n## 단어 추출 및 한글 의미 리스트:\n\n*   **Sequences:** 시퀀스, 연속된 것, 순서\n*   **Pose:** 제기하다, 야기하다\n*   **Challenge:** 어려움, 난관, 도전\n*   **DNNs:** 심층 신경망 (Deep Neural Networks)\n*   **Dimensionality:** 차원\n*   **Inputs:** 입력\n*   **Outputs:** 출력\n*   **Known:** 알려진\n*   **Fixed:** 고정된\n*   **Straightforward:** 간단한, 직관적인\n*   **Application:** 적용, 응용\n*   **Long Short-Term Memory (LSTM):** 장단기 기억 (인공 신경망의 한 종류)\n*   **Architecture:** 아키텍처, 구조\n*   **Solve:** 해결하다\n*   **General:** 일반적인, 보편적인\n*   **Sequence to sequence:** 시퀀스-투-시퀀스 (기계 번역 등에서 입력 시퀀스를 출력 시퀀스로 변환하는 방식)\n*   **Idea:** 아이디어, 생각\n*   **Read:** 읽다\n*   **Timestep:** 타임스텝, 시간 단계\n*   **Obtain:** 얻다\n*   **Large:** 큰\n*   **Vector representation:** 벡터 표현\n*   **Extract:** 추출하다\n*   **Essentially:** 기본적으로, 본질적으로\n*   **Recurrent neural network:** 순환 신경망\n*   **Language model:** 언어 모델\n*   **Conditioned:** 조건화된\n*   **Ability:** 능력\n*   **Successfully:** 성공적으로\n*   **Learn:** 배우다, 학습하다\n*   **Data:** 데이터\n*   **Long range temporal dependencies:** 장거리 시간 의존성\n*   **Natural:** 자연스러운\n*   **Choice:** 선택\n*   **Considerable:** 상당한\n*   **Time lag:** 시간차\n*   **Corresponding:** 해당되는, 상응하는\n*   **Problems:** 문제들"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and fixed.\n### 번역: 시퀀스 데이터는 입력과 출력의 차원이 미리 알려져 있고 고정되어 있어야 하기 때문에 DNN(심층 신경망)에게는 어려운 과제를 제시합니다.\n---\n\n\n### 원문: In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\n### 번역: 본 논문에서는 LSTM(Long Short-Term Memory) 구조 [16]를 직접적으로 적용하여 일반적인 시퀀스-투-시퀀스 문제를 해결할 수 있음을 보여줍니다.\n---\n\n\n### 원문: The idea is to use one LSTMto read the input sequence, one timestep at a time, to obtain large fixeddimensional\nvector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (fig. 1).\n### 번역: 아이디어는 하나의 LSTM을 사용하여 입력 시퀀스를 한 타임스텝씩 읽어 들여 고정된 차원의 큰 벡터 표현을 얻고, 다른 LSTM을 사용하여 그 벡터로부터 출력 시퀀스를 추출하는 것입니다 (그림 1).\n---\n\n\n### 원문: The second LSTMis essentially a recurrent neural network languagemodel\n[28, 23, 30] except that it is conditioned on the input sequence.\n### 번역: 두 번째 LSTM은 기본적으로 순환 신경망 언어 모델 [28, 23, 30]과 같지만, 입력 시퀀스에 의해 조건화된다는 점이 다릅니다.\n---\n\n\n### 원문: The LSTM’s ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (fig. 1).\n### 번역: LSTM은 장기적인 시간 의존성을 가진 데이터에 대해 성공적으로 학습할 수 있는 능력이 있기 때문에, 입력과 해당 출력 간에 상당한 시간 지연이 존재하는 이러한 응용 분야에 자연스러운 선택입니다 (그림 1).\n---\n\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n신경망을 이용한 일반적인 시퀀스-투-시퀀스 학습 문제 해결을 위한 다양한 관련 시도가 있었습니다. 우리의 접근 방식은 전체 입력 문장을 벡터로 매핑한 Kalchbrenner와 Blunsom [18]의 연구와 밀접하게 관련되어 있으며, Cho et al. [5]의 연구와도 관련이 있지만, 후자는 구문 기반 시스템에서 생성된 가설의 재채점에만 사용되었습니다. Graves [10]는 신경망이 입력의 서로 다른 부분에 집중할 수 있도록 하는 새로운 미분 가능한 어텐션 메커니즘을 도입했으며, Bahdanau et al. [2]은 이 아이디어의 우아한 변형을 기계 번역에 성공적으로 적용했습니다. Connectionist Sequence Classification은 신경망을 사용하여 시퀀스를 시퀀스로 매핑하는 또 다른 인기 있는 기술이지만, 입력과 출력 간의 단조로운 정렬을 가정합니다 [11].\n\n## 추출된 단어 및 한글 의미:\n\n*   **there:** 있다, 존재하다\n*   **have:** 가지다, 있다\n*   **been:** (have 동사의 과거 분사형) ~였다, ~되어 왔다\n*   **a:** (부정관사) 하나의, 어느\n*   **number:** 숫자, 수, 횟수\n*   **of:** ~의, ~중에서\n*   **related:** 관련된, 연관된\n*   **attempts:** 시도, 노력\n*   **to:** ~에, ~로, ~하기 위해\n*   **address:** 다루다, 해결하다\n*   **the:** (정관사) 그, 저\n*   **general:** 일반적인, 보편적인\n*   **sequence:** 시퀀스, 연속\n*   **learning:** 학습\n*   **problem:** 문제\n*   **with:** ~와 함께, ~으로\n*   **neural:** 신경의, 신경망의\n*   **networks:** 네트워크, 망, (신경)망\n*   **our:** 우리의\n*   **approach:** 접근 방식, 방법\n*   **closely:** 밀접하게, 가까이\n*   **Kalchbrenner:** 칼츠브레너 (이름)\n*   **and:** 그리고\n*   **Blunsom:** 블룬섬 (이름)\n*   **who:** ~하는 사람, 누구\n*   **were:** (be 동사의 과거 복수형) ~였다\n*   **first:** 처음, 첫 번째\n*   **map:** 매핑하다, 지도\n*   **entire:** 전체의, 완전한\n*   **input:** 입력\n*   **sentence:** 문장\n*   **vector:** 벡터\n*   **is:** ~이다\n*   **Cho:** 조 (이름)\n*   **et al.:** 외 (라틴어 abbreviation: et alii 의 약자), 기타 등등\n*   **although:** ~에도 불구하고\n*   **latter:** 후자\n*   **was:** (be 동사의 과거 단수형) ~였다\n*   **used:** 사용된\n*   **only:** 오직, 단지\n*   **for:** ~을 위해, ~에 대해\n*   **rescoring:** 재채점\n*   **hypotheses:** 가설 (hypothesis의 복수형)\n*   **produced:** 생산된, 생성된\n*   **by:** ~에 의해\n*   **phrase-based:** 구문 기반의\n*   **system:** 시스템, 체계\n*   **Graves:** 그레이브스 (이름)\n*   **introduced:** 도입된, 소개된\n*   **novel:** 새로운, 참신한\n*   **differentiable:** 미분 가능한\n*   **attention:** 어텐션, 주의 집중\n*   **mechanism:** 메커니즘, 기계 장치\n*   **that:** 저, ~하는 것\n*   **allows:** 허용하다, 가능하게 하다\n*   **to focus:** 집중하다\n*   **different:** 다른, 서로 다른\n*   **parts:** 부분, 부품\n*   **their:** 그들의\n*   **an:** (부정관사) 하나의, 어느 (모음으로 시작하는 단어 앞에서 사용)\n*   **elegant:** 우아한, 세련된\n*   **variant:** 변형, 이형\n*   **this:** 이것\n*   **idea:** 아이디어, 생각\n*   **successfully:** 성공적으로\n*   **applied:** 적용된\n*   **machine translation:** 기계 번역\n*   **Bahdanau:** 바흐다나우 (이름)\n*   **Connectionist:** 연결주의적\n*   **Sequence Classification:** 시퀀스 분류\n*   **another:** 또 다른, 다른\n*   **popular:** 인기 있는, 대중적인\n*   **technique:** 기술, 기법\n*   **mapping:** 매핑, 사상\n*   **but:** 하지만, 그러나\n*   **it:** 그것\n*   **assumes:** 가정하다, 추정하다\n*   **monotonic:** 단조로운\n*   **alignment:** 정렬, 배열\n*   **between:** ~사이에, ~간에\n*   **outputs:** 출력\n\nThis list provides a comprehensive breakdown of the words used in the text and their Korean meanings.  It should be helpful for understanding the technical terms and concepts discussed in the original text.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n신경망을 이용하여 일반적인 시퀀스-투-시퀀스 학습 문제를 해결하려는 다양한 시도가 있었습니다. 우리의 접근 방식은 전체 입력 문장을 벡터로 매핑한 최초의 연구자인 Kalchbrenner와 Blunsom [18]의 연구와 밀접한 관련이 있으며, Cho et al. [5]의 연구와도 관련이 있습니다. 다만 후자는 구문 기반 시스템에서 생성된 가설의 재평가에만 사용되었습니다. Graves [10]는 신경망이 입력의 서로 다른 부분에 집중할 수 있도록 하는 새로운 미분 가능한 어텐션 메커니즘을 도입했으며, Bahdanau et al. [2]에 의해 이 아이디어의 우아한 변형이 기계 번역에 성공적으로 적용되었습니다. Connectionist Sequence Classification은 신경망을 사용하여 시퀀스를 시퀀스로 매핑하는 또 다른 인기 있는 기술이지만, 입력과 출력 사이에 단조로운 정렬을 가정합니다 [11].\n\n## 단어 및 의미 리스트\n\n*   **There:** 있다, 존재하다\n*   **Have:** 가지다, 있다 (조동사로 사용)\n*   **Been:** 있다 (be 동사의 과거 분사)\n*   **A:** 하나, 어떤\n*   **Number:** 수, 숫자\n*   **Of:** ~의\n*   **Related:** 관련된, 관련있는\n*   **Attempts:** 시도, 노력\n*   **To:** ~에게, ~로, ~하기 위해\n*   **Address:** 다루다, 해결하다\n*   **The:** 그, 저\n*   **General:** 일반적인, 보편적인\n*   **Sequence:** 순서, 시퀀스\n*   **Learning:** 학습, 배움\n*   **Problem:** 문제\n*   **With:** ~와 함께, ~을 가지고\n*   **Neural:** 신경의\n*   **Networks:** 네트워크, 망\n*   **Our:** 우리의\n*   **Approach:** 접근 방식, 방법\n*   **Closely:** 밀접하게, 가까이\n*   **Kalchbrenner:** (고유명사, 연구자 이름)\n*   **And:** 그리고, ~와\n*   **Blunsom:** (고유명사, 연구자 이름)\n*   **Who:** 누구, ~하는 사람\n*   **Were:** ~이었다 (be 동사의 과거 복수형)\n*   **First:** 처음의, 최초의\n*   **Map:** 매핑하다, 지도\n*   **Entire:** 전체의, 완전한\n*   **Input:** 입력\n*   **Sentence:** 문장\n*   **Vector:** 벡터\n*   **Is:** ~이다 (be 동사의 3인칭 단수 현재형)\n*   **Cho:** (고유명사, 연구자 이름)\n*   **Although:** 비록 ~이지만\n*   **Latter:** 후자\n*   **Was:** ~이었다 (be 동사의 과거 단수형)\n*   **Used:** 사용된, 쓰인\n*   **Only:** 오직, 단지\n*   **For:** ~을 위해, ~에 대해\n*   **Rescoring:** 재평가\n*   **Hypotheses:** 가설 (hypothesis의 복수형)\n*   **Produced:** 생산된, 만들어진\n*   **By:** ~에 의해, ~로\n*   **A:** 하나의, 어떤\n*   **Phrase-based:** 구문 기반의\n*   **System:** 시스템, 체계\n*   **Graves:** (고유명사, 연구자 이름)\n*   **Introduced:** 도입된, 소개된\n*   **Novel:** 새로운, 참신한\n*   **Differentiable:** 미분 가능한\n*   **Attention:** 어텐션, 주의 집중\n*   **Mechanism:** 메커니즘, 기제\n*   **That:** ~하는 것, 저것\n*   **Allows:** 허용하다, 가능하게 하다\n*   **To:** ~에게, ~로, ~하기 위해\n*   **Focus:** 집중하다\n*   **On:** ~에, ~위에\n*   **Different:** 다른, 상이한\n*   **Parts:** 부분, 부위\n*   **Their:** 그들의\n*   **An:** 하나의 (모음 앞에서 사용)\n*   **Elegant:** 우아한, 세련된\n*   **Variant:** 변형, 이형\n*   **This:** 이것\n*   **Idea:** 아이디어, 생각\n*   **Successfully:** 성공적으로\n*   **Applied:** 적용된, 응용된\n*   **Machine:** 기계\n*   **Translation:** 번역\n*   **Bahdanau:** (고유명사, 연구자 이름)\n*   **Connectionist:** 연결주의의\n*   **Classification:** 분류, 범주화\n*   **Another:** 또 다른\n*   **Popular:** 인기 있는\n*   **Technique:** 기술, 기법\n*   **Mapping:** 매핑, 사상\n*   **But:** 그러나, 하지만\n*   **It:** 그것\n*   **Assumes:** 가정하다, 추정하다\n*   **Monotonic:** 단조로운\n*   **Alignment:** 정렬, 배열\n*   **Between:** ~사이에\n   **Outputs:** 출력\n\n**참고:**\n\n*   고유 명사 (연구자 이름 등)는 특별한 의미 변화 없이 그대로 적었습니다.\n*   문맥에 따라 의미가 조금씩 달라질 수 있습니다.\n*   간단한 단어 (a, the, is 등)도 포함했습니다.\n*   동사의 경우 기본형으로 제시했습니다."
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks.\n### 번역: 신경망을 이용한 일반적인 시퀀스-투-시퀀스 학습 문제 해결을 위한 여러 관련 시도가 있었습니다.\n---\n\n\n### 원문: Our approach is closely related to Kalchbrenner and Blunsom [18] who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system.\n### 번역: 우리의 접근 방식은 전체 입력 문장을 벡터로 매핑한 최초의 연구인 Kalchbrenner와 Blunsom [18]의 연구와 밀접하게 관련되어 있으며, Cho 외 [5]의 연구와도 관련이 있지만 후자는 구문 기반 시스템에서 생성된 가설의 재평가에만 사용되었습니다.\n---\n\n\n### 원문: Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on different parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al. [2].\n### 번역: Graves [10]는 신경망이 입력의 여러 부분에 집중할 수 있도록 하는 새로운 미분 가능한 어텐션 메커니즘을 도입했으며, 이 아이디어의 정교한 변형이 Bahdanau 외 [2]에 의해 기계 번역에 성공적으로 적용되었습니다.\n---\n\n\n### 원문: The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, but it assumes a monotonic alignment between the inputs and the outputs [11].\n### 번역: 연결주의 시퀀스 분류는 신경망을 사용하여 시퀀스를 시퀀스로 매핑하는 또 다른 인기 있는 기술이지만, 입력과 출력 간의 단조 정렬을 가정합니다 [11].\n---\n\n\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks.\n### 번역: 신경망을 이용한 일반적인 시퀀스-투-시퀀스 학습 문제를 해결하기 위한 여러 관련 시도가 있었습니다.\n---\n\n\n### 원문: Our approach is closely related to Kalchbrenner and Blunsom [18] who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system.\n### 번역: 저희 접근 방식은 입력 문장 전체를 벡터로 매핑한 최초의 연구자인 Kalchbrenner와 Blunsom [18]의 연구와 밀접한 관련이 있으며, Cho et al. [5]의 연구와도 관련이 있지만, 후자는 구문 기반 시스템에서 생성된 가설의 재평가에만 사용되었습니다.\n---\n\n\n### 원문: Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on different parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al. [2].\n### 번역: Graves [10]는 신경망이 입력의 여러 부분에 집중할 수 있도록 하는 새로운 미분 가능한 어텐션 메커니즘을 도입했으며, Bahdanau et al. [2]은 이 아이디어의 세련된 변형을 기계 번역에 성공적으로 적용했습니다.\n---\n\n\n### 원문: The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, but it assumes a monotonic alignment between the inputs and the outputs [11].\n### 번역: Connectionist Sequence Classification은 신경망을 사용하여 시퀀스를 시퀀스로 매핑하는 또 다른 인기 있는 기술이지만, 입력과 출력 간의 단조 정렬을 가정합니다 [11].\n---\n\n\n"
    }
  ]
}