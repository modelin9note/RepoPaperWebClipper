{
  "name": "MedGemmaTR",
  "timestamp": "2025-07-14T08:09:55.007Z",
  "responses": [
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n**MedGemma 기술 보고서**\n\nGoogle Research 및 Google DeepMind 1\n\n인공지능(AI)은 의료 분야에서 상당한 잠재력을 가지고 있지만, 의료 데이터의 다양성, 가능한 작업의 복잡한 스펙트럼, 프라이버시 보호의 필요성으로 인해 학습 및 배포에 어려움이 있습니다. 다양한 의료 작업에서 뛰어난 성능을 보이고 작업별 튜닝 데이터가 덜 필요한 기초 모델은 의료 애플리케이션을 위한 AI 개발을 가속화하는 데 중요합니다. 이 기술 보고서에서는 Gemma 3 4B 및 27B를 기반으로 한 새로운 의료 시각-언어 기초 모델 모음인 MedGemma를 소개합니다. MedGemma는 이미지와 텍스트에 대한 고급 의료 이해 및 추론 능력을 보여주며, 유사한 크기의 생성 모델의 성능을 크게 능가하고 작업별 모델의 성능에 근접하면서 Gemma 3 기본 모델의 일반적인 기능을 유지합니다. 분포 외(out-of-distribution) 작업의 경우, MedGemma는 의료 다중 모드 질문 답변에서 2.6-10% 향상, 흉부 X-레이 소견 분류에서 15.5-18.1% 향상, 에이전트 평가에서 기본 모델에 비해 10.8% 향상을 달성했습니다. MedGemma를 미세 조정하면 하위 도메인에서 성능이 더욱 향상되어 전자 건강 기록 정보 검색 오류를 50% 줄이고 기흉 분류 및 조직병리학 패치 유형 분류에 대한 기존의 전문화된 최첨단 방법과 비슷한 성능에 도달합니다. 또한 SigLIP에서 파생된 의학적으로 조정된 시각 인코더인 MedSigLIP을 소개합니다. MedSigLIP은 MedGemma의 시각적 이해 능력을 강화하며, 인코더로서 전문 의료 이미지 인코더와 비슷하거나 더 나은 성능을 달성합니다. 종합적으로 MedGemma 컬렉션은 의료 이미지 및 텍스트 기능의 강력한 기반을 제공하여 의료 연구 및 다운스트림 애플리케이션 개발을 크게 가속화할 가능성이 있습니다. 모델 가중치 다운로드를 위한 튜토리얼 및 지침을 포함한 MedGemma 컬렉션에 대한 자세한 내용은 https://goo.gle/medgemma 에서 확인할 수 있습니다.\n1\n## 단어 및 의미 리스트:\n\n*   **MedGemma:** 메드젬마 (이 보고서에서 소개하는 의료 시각-언어 기초 모델의 이름)\n*   **Technical Report:** 기술 보고서 (특정 기술 주제에 대한 정보와 분석을 제공하는 문서)\n*   **Google Research:** 구글 리서치 (구글의 연구 부서)\n*   **Google DeepMind:** 구글 딥마인드 (구글의 AI 연구 개발 회사)\n*   **Artificial intelligence (AI):** 인공지능 (인간의 지능을 모방하는 컴퓨터 시스템)\n*   **Healthcare:** 의료 (건강 관리 및 질병 치료)\n*   **Applications:** 애플리케이션 (특정 목적을 위해 설계된 소프트웨어 또는 프로그램)\n*   **Training:** 학습 (모델이 데이터를 기반으로 패턴을 배우는 과정)\n*   **Deployment:** 배포 (학습된 모델을 실제 환경에서 사용하는 과정)\n*   **Data:** 데이터 (정보의 조각 또는 사실)\n*   **Tasks:** 작업 (수행해야 할 특정 활동 또는 목표)\n*   **Privacy:** 프라이버시 (개인 정보 보호)\n*   **Foundation models:** 기초 모델 (사전 학습된 대규모 모델로, 특정 작업에 맞게 미세 조정될 수 있음)\n*   **Medical:** 의료의, 의학적인\n*   **Vision-language:** 시각-언어 (이미지와 텍스트를 함께 처리하는 기술)\n*   **Gemma:** 젬마 (MedGemma의 기반이 되는 구글의 언어 모델)\n*   **Understanding:** 이해 (정보를 파악하고 해석하는 능력)\n*   **Reasoning:** 추론 (주어진 정보를 바탕으로 논리적인 결론을 도출하는 과정)\n*   **Images:** 이미지 (그림, 사진 등의 시각적 표현)\n*   **Text:** 텍스트 (문자열로 표현된 정보)\n*   **Generative models:** 생성 모델 (새로운 데이터를 생성할 수 있는 모델)\n*   **Capabilities:** 기능 (특정 작업을 수행할 수 있는 능력)\n*   **Base models:** 기본 모델 (미세 조정되기 전의 모델)\n*   **Out-of-distribution:** 분포 외 (학습 데이터와 다른 분포를 가진 데이터)\n*   **Multimodal:** 다중 모드 (여러 종류의 데이터를 함께 사용하는 것, 예: 이미지와 텍스트)\n*   **Question answering:** 질문 답변 (질문에 대한 답을 찾는 작업)\n*   **Chest X-ray:** 흉부 X-레이 (가슴 부위를 촬영한 X선 사진)\n*   **Classification:** 분류 (데이터를 특정 범주로 나누는 작업)\n*   **Agentic evaluations:** 에이전트 평가 (AI 에이전트의 성능을 평가하는 것)\n*   **Fine-tuning:** 미세 조정 (사전 학습된 모델을 특정 작업에 맞게 추가 학습시키는 과정)\n*   **Subdomains:** 하위 도메인 (특정 분야의 더 작은 부분)\n*   **Electronic health record:** 전자 건강 기록 (환자의 건강 관련 정보를 디지털 형태로 저장한 기록)\n*   **Information retrieval:** 정보 검색 (필요한 정보를 찾아내는 과정)\n*   **State-of-the-art:** 최첨단 (현재 가장 발전된 기술 또는 방법)\n*   **Pneumothorax:** 기흉 (폐에 공기가 차는 질병)\n*   **Histopathology:** 조직병리학 (질병 진단을 위해 조직을 현미경으로 검사하는 학문)\n*   **Patch:** 패치 (이미지의 작은 조각)\n*   **Type:** 유형 (데이터의 종류)\n*   **Encoder:** 인코더 (데이터를 특정 형식으로 변환하는 모델)\n*   **Derived:** 파생된 (어떤 것으로부터 만들어진)\n*   **SigLIP:** SigLIP (시각적 표현을 학습하는 데 사용되는 모델)\n*   **MedSigLIP:** 메드시그립 (의학적으로 조정된 SigLIP 모델)\n*   **Visual:** 시각적인\n*   **Medical image:** 의료 영상 (X-레이, MRI 등 의료 목적으로 촬영된 이미지)\n*   **Downstream:** 다운스트림 (기초 모델을 기반으로 개발된 애플리케이션 또는 작업)\n*   **Tutorials:** 튜토리얼 (학습 자료 또는 안내)\n*   **Instructions:** 지침 (작업을 수행하는 방법)\n*   **Model weights:** 모델 가중치 (모델의 학습된 파라미터)\n*   **Downloading:** 다운로드 (인터넷에서 파일을 가져오는 것)\n*   **https://goo.gle/medgemma:** 모델 및 관련 정보를 다운로드할 수 있는 링크입니다.\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: MedGemma Technical Report\n### 번역: MedGemma 기술 보고서\n---\n\n\n### 원문: Google Research and Google DeepMind 1\n### 번역: Google Research 및 Google DeepMind 1\n---\n\n\n### 원문: Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment\nare challenging due to healthcare’s diverse data, complex spectrum of possible tasks, and the\nneed to preserve privacy.\n### 번역: 인공지능(AI)은 의료 분야에서 상당한 잠재력을 가지고 있지만, 의료 데이터의 다양성, 가능한 작업의 복잡한 스펙트럼, 그리고 개인 정보 보호의 필요성 때문에 훈련과 배포에 어려움이 있습니다.\n---\n\n\n### 원문: Foundation models that perform well on various medical tasks and require less\ntask-specific tuning data are critical to accelerating the development of AI for healthcare applications.\n### 번역: 다양한 의료 작업에서 뛰어난 성능을 보이고 작업별 튜닝 데이터가 적게 필요한 기초 모델은 의료 분야 AI 개발을 가속화하는 데 매우 중요합니다.\n---\n\n\n### 원문: In\nthis technical report, we introduce MedGemma, a new collection of medical vision–language foundation\nmodels based on Gemma 3 4B and 27B.\n### 번역: 본 기술 보고서에서는 Gemma 3 4B 및 27B를 기반으로 하는 새로운 의료 비전-언어 기초 모델 모음인 MedGemma를 소개합니다.\n---\n\n\n### 원문: MedGemma demonstrates advanced medical understanding and\nreasoning on images and text, significantly exceeding the performance of similar-sized generative models\nand approaching the performance of task-specific models, while maintaining the general capabilities of\nthe Gemma 3 base models.\n### 번역: MedGemma는 이미지와 텍스트에 대한 고도의 의료 이해 및 추론 능력을 보여주며, 유사한 크기의 생성 모델 성능을 훨씬 능가하고 작업별 모델의 성능에 근접하면서도 Gemma 3 기본 모델의 일반적인 기능은 유지합니다.\n---\n\n\n### 원문: For out-of-distribution tasks, MedGemma achieves 2.6-10% improvements on\nmedical multimodal question answering, 15.5-18.1% improvements on chest X-ray finding classification,\nand 10.8% improvement on agentic evaluations compared to the base models.\n### 번역: 분포 외(Out-of-distribution) 작업에서 MedGemma는 기본 모델에 비해 의료 멀티모달 질의 응답에서 2.6-10% 향상, 흉부 X선 소견 분류에서 15.5-18.1% 향상, 에이전트 평가에서 10.8% 향상을 달성했습니다.\n---\n\n\n### 원문: Fine-tuning MedGemma\nfurther improves performance in subdomains, reducing errors in electronic health record information\nretrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods\nfor pneumothorax classification and histopathology patch type classification.\n### 번역: MedGemma를 미세 조정하면 하위 도메인에서 성능이 더욱 향상되어 전자 건강 기록 정보 검색 오류를 50% 줄이고 기흉 분류 및 조직병리학 패치 유형 분류에서 기존의 특화된 최첨단 방법과 비슷한 성능을 달성합니다.\n---\n\n\n### 원문: We additionally introduce\nMedSigLIP, a medically-tuned vision encoder derived from SigLIP.\n### 번역: 또한 SigLIP에서 파생된 의료적으로 튜닝된 비전 인코더인 MedSigLIP을 소개합니다.\n---\n\n\n### 원문: MedSigLIP powers the visual\nunderstanding capabilities of MedGemma and, as an encoder, it achieves performance comparable to or\nbetter than specialized medical image encoders.\n### 번역: MedSigLIP은 MedGemma의 시각적 이해 능력을 강화하며, 인코더로서 특화된 의료 이미지 인코더와 비슷하거나 더 나은 성능을 달성합니다.\n---\n\n\n### 원문: Taken together, the MedGemma collection provides\na strong foundation of medical image and text capabilities, with potential to significantly accelerate\nmedical research and development of downstream applications.\n### 번역: 종합적으로 볼 때, MedGemma 컬렉션은 의료 이미지 및 텍스트 기능의 강력한 기반을 제공하며, 의료 연구와 다운스트림 애플리케이션 개발을 크게 가속화할 가능성이 있습니다.\n---\n\n\n### 원문: More details about the MedGemma\ncollection, including tutorials and instructions for downloading the model weights, can be found at\nhttps://goo.gle/medgemma.\n### 번역: 튜토리얼 및 모델 가중치 다운로드 지침을 포함한 MedGemma 컬렉션에 대한 자세한 내용은 https://goo.gle/medgemma 에서 확인할 수 있습니다.\n---\n\n\n### 원문: 1\n### 번역: 1\n---\n\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n**1. 서론**\n\n현대 의료 환경은 전례 없는 양과 다양성의 데이터 생성 및 사용을 특징으로 합니다. 진단, 치료 및 모니터링은 서로 다른 출처와 전문 분야로부터 정보를 종합하는 데 의존합니다. 최근 개발된 대규모 멀티모달 모델(LMM)은 방대하고 다양한 데이터 세트에서 훈련되어 복잡한 패턴을 감지하고, 일관성 있는 텍스트를 생성하고, 시각 정보를 처리하는 데 뛰어난 능력을 보여줍니다 (Achiam et al., 2023; Alayrac et al., 2022; Chen et al., 2022; Liu et al., 2023, 2024; OpenAI, 2023; Touvron et al., 2023). 이러한 능력은 현재 워크플로우를 지원하고 새로운 통찰력을 추출하는 데 있어 잠재적인 패러다임 전환을 의미합니다. 범용(의학적으로 조정되지 않은) LMM이 매우 광범위한 능력을 보여주지만, 일반 모델은 미묘한 의학적 이해와 의학적 데이터를 강력하게 해석하고 추론하는 능력이 부족할 수 있습니다 (Han et al., 2023; Labrak et al., 2024; Singhal et al., 2023b,c; Toma et al., 2023; Tu et al., 2024; Yang et al., 2024). 이러한 격차를 인식하여 우리는 새로운 오픈 소스, 의학적으로 조정된 비전-언어 기반 모델인 MedGemma를 만들었습니다. 이 모델은 Health AI Developer Foundations (Kiraly et al., 2024) 컬렉션에 가장 최근에 추가된 것입니다. Gemma 3 (Gemma-Team et al., 2025)의 견고한 아키텍처를 기반으로 구축된 MedGemma 모델은 Gemma 3에 존재하는 강력한 범용 기능을 유지하면서 의료 이미지와 텍스트를 해석하고 추론하도록 설계되었습니다.\n\n## 단어 및 의미 리스트\n\n*   **Introduction:** 서론, 소개\n*   **Landscape:** 환경, 지형, 양상\n*   **Modern:** 현대의\n*   **Healthcare:** 의료\n*   **Characterized:** 특징지어진\n*   **Generation:** 생성\n*   **Use:** 사용\n*   **Unprecedented:** 전례 없는\n*   **Volume:** 양, 용량\n*   **Diversity:** 다양성\n*   **Data:** 데이터\n*   **Diagnosis:** 진단\n*   **Treatment:** 치료\n*   **Monitoring:** 모니터링\n*   **Rely:** 의존하다\n*   **Synthesizing:** 종합하는\n*   **Information:** 정보\n*   **Disparate:** 서로 다른\n*   **Sources:** 출처\n*   **Specialties:** 전문 분야\n*   **Recently:** 최근에\n*   **Developed:** 개발된\n*   **Large:** 큰, 대규모의\n*   **Multimodal:** 멀티모달의 (다양한 양식을 사용하는)\n*   **Models:** 모델\n*   **Trained:** 훈련된\n*   **Massive:** 방대한\n*   **Datasets:** 데이터 세트\n*   **Exhibit:** 보여주다, 나타내다\n*   **Remarkable:** 뛰어난, 주목할 만한\n*   **Capabilities:** 능력\n*   **Detecting:** 감지하는\n*   **Complex:** 복잡한\n*   **Patterns:** 패턴\n*   **Generating:** 생성하는\n*   **Coherent:** 일관성 있는\n*   **Text:** 텍스트, 글\n*   **Processing:** 처리하는\n*   **Visual:** 시각적인\n*   **Capabilities:** 능력\n*   **Mark:** 의미하다, 나타내다\n*   **Potential:** 잠재적인\n*   **Paradigm:** 패러다임\n*   **Shift:** 전환\n*   **Assisting:** 지원하는\n*   **Current:** 현재의\n*   **Workflows:** 워크플로우\n*   **Extracting:** 추출하는\n*   **Novel:** 새로운\n*   **Insights:** 통찰력\n*   **General-purpose:** 범용의\n*   **Non-medically:** 의학적으로 ~않은\n*   **Tuned:** 조정된\n*   **Demonstrate:** 보여주다, 입증하다\n*   **Impressively:** 인상적으로\n*   **Broad:** 광범위한\n*   **Abilities:** 능력\n*   **Generic:** 일반적인\n*   **Lack:** 부족하다\n*   **Nuanced:** 미묘한\n*   **Medical:** 의학적인\n*   **Understanding:** 이해\n*   **Ability:** 능력\n*   **Interpret:** 해석하다\n*   **Reason:** 추론하다\n*   **Robust:** 강력한, 견고한\n*   **Recognizing:** 인식하는\n*   **Gap:** 격차\n*   **Created:** 만들었다\n*   **Suite:** 세트, 모음\n*   **Open:** 공개된, 오픈 소스의\n*   **Medically-tuned:** 의학적으로 조정된\n*   **Vision-language:** 비전-언어 (이미지와 텍스트를 함께 사용하는)\n*   **Foundation:** 기반\n*   **Addition:** 추가\n*   **Collection:** 컬렉션\n*   **Built:** 구축된\n*   **Robust:** 견고한\n*   **Architecture:** 아키텍처, 구조\n*   **Designed:** 설계된\n*   **Retaining:** 유지하는\n*   **Strong:** 강한, 강력한\n*   **Present:** 존재하는\n\n이 리스트는 텍스트에 사용된 주요 단어와 그 의미를 포함하고 있습니다. 필요에 따라 추가적인 단어를 분석할 수 있습니다.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n**1. 서론**\n\n현대 의료 환경은 전례 없는 양과 다양성을 가진 데이터의 생성 및 사용으로 특징지어집니다. 진단, 치료, 모니터링은 이질적인 출처와 전문 분야의 정보를 종합하는 데 의존합니다. 최근 개발된 대규모 멀티모달 모델(LMM)은 방대하고 다양한 데이터 세트로 학습되어 복잡한 패턴을 감지하고, 일관성 있는 텍스트를 생성하고, 시각 정보를 처리하는 데 놀라운 능력을 보여줍니다 (Achiam et al., 2023; Alayrac et al., 2022; Chen et al., 2022; Liu et al., 2023, 2024; OpenAI, 2023; Touvron et al., 2023). 이러한 능력은 현재 워크플로우를 지원하고 새로운 통찰력을 추출하는 데 잠재적인 패러다임 전환을 의미합니다. 일반적인 목적(의학적으로 조정되지 않은) LMM은 인상적으로 넓은 능력을 보여주지만, 범용 모델은 미묘한 의학적 이해와 의학 데이터를 강력하게 해석하고 추론하는 능력이 부족할 수 있습니다 (Han et al., 2023; Labrak et al., 2024; Singhal et al., 2023b,c; Toma et al., 2023; Tu et al., 2024; Yang et al., 2024). 이러한 격차를 인지하고, 우리는 MedGemma라는 새로운 오픈 소스, 의학적으로 조정된, 비전-언어 기반 모델 모음을 만들었습니다. 이러한 모델은 Health AI Developer Foundations (Kiraly et al., 2024) 컬렉션에 최신으로 추가된 것입니다. Gemma 3 (Gemma-Team et al., 2025)의 강력한 아키텍처를 기반으로 구축된 MedGemma 모델은 Gemma 3에 존재하는 강력한 범용 기능을 유지하면서 의학 이미지와 텍스트를 해석하고 추론하도록 설계되었습니다.\n\n## 원문 단어 및 한글 의미\n\n다음은 원문에서 추출한 단어와 그에 대한 한글 의미입니다.  맥락에 따라 여러 의미가 있을 수 있지만, 여기서는 가장 적절한 의미를 제시합니다.\n\n*   **Introduction:** 서론, 소개\n*   **landscape:** 환경, 상황, 지형\n*   **modern:** 현대의\n*   **healthcare:** 의료 (서비스)\n*   **characterized:** 특징지어지다\n*   **generation:** 생성, 발생\n*   **unprecedented:** 전례 없는\n*   **volume:** 양, 용량\n*   **diversity:** 다양성\n*   **data:** 데이터\n*   **diagnosis:** 진단\n*   **treatment:** 치료\n*   **monitoring:** 모니터링, 감시\n*   **rely:** 의존하다\n*   **synthesizing:** 종합하다, 통합하다\n*   **information:** 정보\n*   **disparate:** 이질적인, 서로 다른\n*   **sources:** 출처, 근원\n*   **specialties:** 전문 분야\n*   **recently:** 최근에\n*   **developed:** 개발된\n*   **large:** 큰, 대규모의\n*   **multimodal:** 멀티모달 (다양한 형태의 정보)\n*   **models:** 모델\n*   **trained:** 훈련된, 학습된\n*   **massive:** 방대한, 거대한\n*   **datasets:** 데이터 세트\n*   **exhibit:** 보여주다, 나타내다\n*   **remarkable:** 놀라운, 주목할 만한\n*   **capabilities:** 능력\n*   **detecting:** 감지하는\n*   **complex:** 복잡한\n*   **patterns:** 패턴\n*   **generating:** 생성하는\n*   **coherent:** 일관성 있는\n*   **text:** 텍스트, 글\n*   **processing:** 처리하는\n*   **visual:** 시각적인\n*   **capabilities:** 능력\n*   **mark:** 나타내다, 의미하다\n*   **potential:** 잠재적인\n*   **paradigm shift:** 패러다임 전환\n*   **assisting:** 지원하는, 돕는\n*   **current:** 현재의\n*   **workflows:** 워크플로우 (작업 흐름)\n*   **extracting:** 추출하는\n*   **novel:** 새로운, 참신한\n*   **insights:** 통찰력\n*   **general-purpose:** 범용적인\n*   **tuned:** 조정된, 최적화된\n*   **demonstrate:** 보여주다, 입증하다\n*   **impressively:** 인상적으로\n*   **broad:** 넓은, 광범위한\n*   **abilities:** 능력\n*   **generic:** 일반적인, 포괄적인\n*   **lack:** 부족하다\n*   **nuanced:** 미묘한, 뉘앙스가 있는\n*   **medical:** 의학적인\n*   **understanding:** 이해\n*   **interpret:** 해석하다\n*   **reason:** 추론하다, 판단하다\n*   **robust:** 강력한, 견고한\n*   **recognizing:** 인지하는, 인식하는\n*   **gap:** 격차, 간극\n*   **created:** 생성된, 만들어진\n*   **suite:** 모음, 세트\n*   **open:** 오픈 소스\n*   **vision-language:** 시각-언어\n*   **foundation:** 기반\n*   **addition:** 추가\n*   **collection:** 컬렉션, 모음\n*   **built:** 구축된, 건설된\n*   **architecture:** 아키텍처, 구조\n*   **designed:** 설계된\n*   **retain:** 유지하다\n*   **present:** 존재하는"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 1. Introduction\n### 번역: 1. 서론\n---\n\n\n### 원문: The landscape of modern healthcare is characterized by the generation and use of an unprecedented\nvolume and diversity of data.\n### 번역: 현대 의료 환경은 전례 없는 양과 다양성의 데이터 생성 및 사용을 특징으로 합니다.\n---\n\n\n### 원문: Diagnosis, treatment, and monitoring rely on synthesizing information from disparate sources and specialties.\n### 번역: 진단, 치료, 그리고 모니터링은 서로 다른 출처와 전문 분야로부터 정보를 종합하는 데 의존합니다.\n---\n\n\n### 원문: Recently developed large multimodal models (LMMs), trained on massive and diverse datasets, exhibit remarkable capabilities in detecting complex patterns,\ngenerating coherent text, and processing visual information (Achiam et al., 2023; Alayrac et al., 2022;\nChen et al., 2022; Liu et al., 2023, 2024; OpenAI, 2023; Touvron et al., 2023).\n### 번역: 최근 개발된 대규모 멀티모달 모델(LMM)은 방대한 양의 다양한 데이터셋으로 훈련되어 복잡한 패턴을 감지하고, 일관성 있는 텍스트를 생성하며, 시각 정보를 처리하는 데 놀라운 능력을 보여줍니다 (Achiam et al., 2023; Alayrac et al., 2022; Chen et al., 2022; Liu et al., 2023, 2024; OpenAI, 2023; Touvron et al., 2023).\n---\n\n\n### 원문: These capabilities\nmark a potential paradigm shift in assisting with current workflows and extracting novel insights.\n### 번역: 이러한 능력은 현재의 업무 흐름을 지원하고 새로운 통찰력을 추출하는 데 있어 잠재적인 패러다임 전환을 의미합니다.\n---\n\n\n### 원문: While general-purpose (non-medically tuned) LMMs demonstrate impressively broad abilities,\ngeneric models can lack nuanced medical understanding and the ability to interpret and reason about\nmedical data in a robust way (Han et al., 2023; Labrak et al., 2024; Singhal et al., 2023b,c; Toma\net al., 2023; Tu et al., 2024; Yang et al., 2024).\n### 번역: 범용(의학적으로 조정되지 않은) LMM이 인상적으로 광범위한 능력을 보여주지만, 일반적인 모델은 미묘한 의학적 이해와 의학 데이터를 강력하게 해석하고 추론하는 능력이 부족할 수 있습니다 (Han et al., 2023; Labrak et al., 2024; Singhal et al., 2023b,c; Toma et al., 2023; Tu et al., 2024; Yang et al., 2024).\n---\n\n\n### 원문: Recognizing this gap, we created MedGemma, a\nnew suite of open, medically-tuned, vision-language foundation models.\n### 번역: 이러한 격차를 인식하고, 우리는 개방형의, 의학적으로 조정된 새로운 비전-언어 파운데이션 모델 스위트인 MedGemma를 만들었습니다.\n---\n\n\n### 원문: These models represent the\nlatest addition to the Health AI Developer Foundations (Kiraly et al., 2024) collection.\n### 번역: 이 모델들은 Health AI Developer Foundations (Kiraly et al., 2024) 컬렉션에 가장 최근에 추가된 것입니다.\n---\n\n\n### 원문: Built upon the\nrobust architecture of Gemma 3 (Gemma-Team et al., 2025), the MedGemma models are designed\nto interpret and reason about medical images and text while retaining the strong general-purpose\ncapabilities present in Gemma 3.\n### 번역: Gemma 3 (Gemma-Team et al., 2025)의 견고한 아키텍처를 기반으로 구축된 MedGemma 모델은 Gemma 3에 존재하는 강력한 범용 기능을 유지하면서 의료 이미지와 텍스트를 해석하고 추론하도록 설계되었습니다.\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 1. Introduction\n### 번역: 1. 서론\n---\n\n\n### 원문: The landscape of modern healthcare is characterized by the generation and use of an unprecedented\nvolume and diversity of data.\n### 번역: 현대 의료 환경은 전례 없는 양과 다양성의 데이터 생성 및 활용을 특징으로 합니다.\n---\n\n\n### 원문: Diagnosis, treatment, and monitoring rely on synthesizing information\nfrom disparate sources and specialties.\n### 번역: 진단, 치료, 그리고 모니터링은 서로 다른 출처와 전문 분야로부터 얻은 정보를 종합하는 데 의존합니다.\n---\n\n\n### 원문: Recently developed large multimodal models (LMMs), trained\non massive and diverse datasets, exhibit remarkable capabilities in detecting complex patterns,\ngenerating coherent text, and processing visual information (Achiam et al., 2023; Alayrac et al., 2022;\nChen et al., 2022; Liu et al., 2023, 2024; OpenAI, 2023; Touvron et al., 2023).\n### 번역: 최근 개발된 대규모 멀티모달 모델(LMMs)은 방대하고 다양한 데이터셋으로 학습되어 복잡한 패턴을 감지하고, 일관성 있는 텍스트를 생성하며, 시각 정보를 처리하는 데 놀라운 능력을 보여줍니다 (Achiam et al., 2023; Alayrac et al., 2022; Chen et al., 2022; Liu et al., 2023, 2024; OpenAI, 2023; Touvron et al., 2023).\n---\n\n\n### 원문: These capabilities\nmark a potential paradigm shift in assisting with current workflows and extracting novel insights.\n### 번역: 이러한 능력들은 현재의 업무 흐름을 지원하고 새로운 통찰력을 추출하는 데 있어 잠재적인 패러다임 변화를 예고합니다.\n---\n\n\n### 원문: While general-purpose (non-medically tuned) LMMs demonstrate impressively broad abilities,\ngeneric models can lack nuanced medical understanding and the ability to interpret and reason about\nmedical data in a robust way (Han et al., 2023; Labrak et al., 2024; Singhal et al., 2023b,c; Toma\net al., 2023; Tu et al., 2024; Yang et al., 2024).\n### 번역: 범용(의학적으로 조정되지 않은) LMM이 인상적으로 광범위한 능력을 보여주지만, 일반 모델은 미묘한 의학적 이해와 의학 데이터를 강력하게 해석하고 추론하는 능력이 부족할 수 있습니다 (Han et al., 2023; Labrak et al., 2024; Singhal et al., 2023b,c; Toma et al., 2023; Tu et al., 2024; Yang et al., 2024).\n---\n\n\n### 원문: Recognizing this gap, we created MedGemma, a\nnew suite of open, medically-tuned, vision-language foundation models.\n### 번역: 이러한 격차를 인식하여, 저희는 새로운 오픈 소스 기반의 의학적으로 조정된 비전-언어 기초 모델인 MedGemma를 개발했습니다.\n---\n\n\n### 원문: These models represent the\nlatest addition to the Health AI Developer Foundations (Kiraly et al., 2024) collection.\n### 번역: 이 모델들은 Health AI Developer Foundations (Kiraly et al., 2024) 컬렉션에 최신으로 추가된 모델입니다.\n---\n\n\n### 원문: Built upon the\nrobust architecture of Gemma 3 (Gemma-Team et al., 2025), the MedGemma models are designed\nto interpret and reason about medical images and text while retaining the strong general-purpose\ncapabilities present in Gemma 3.\n### 번역: Gemma 3 (Gemma-Team et al., 2025)의 강력한 아키텍처를 기반으로 구축된 MedGemma 모델은 Gemma 3에 존재하는 강력한 범용 기능을 유지하면서 의료 이미지와 텍스트를 해석하고 추론하도록 설계되었습니다.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n그림 1 | MedSigLIP 이미지 인코더, MedGemma 4B 멀티모달 모델, 그리고 MedGemma 27B 텍스트 모델을 포함하는 MedGemma 모델 컬렉션 개요\n\n## 단어 추출 및 의미:\n\n*   **Figure:** 그림, 도표\n*   **Overview:** 개요, 요약\n*   **MedGemma:** MedGemma (모델 이름)\n*   **model:** 모델\n*   **collection:** 컬렉션, 모음, 수집\n*   **featuring:** 특징으로 하는, 포함하는\n*   **MedSigLIP:** MedSigLIP (모델 이름)\n*   **image:** 이미지, 그림, 사진\n*   **encoder:** 인코더, 부호기\n*   **Multimodal:** 멀티모달, 다중 모드\n*   **Text:** 텍스트, 글\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: Figure 1 | Overview of the MedGemma model collection featuring the MedSigLIP image encoder, MedGemma 4B Multimodal and MedGemma 27B Text\n### 번역: 그림 1 | MedSigLIP 이미지 인코더, MedGemma 4B 멀티모달, 그리고 MedGemma 27B 텍스트를 특징으로 하는 MedGemma 모델 컬렉션 개요\n---\n\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n이 보고서에서는 두 가지 MedGemma 모델에 집중합니다. 하나는 텍스트, 이미지 또는 둘 다를 입력으로 받을 수 있는 4B 변형 모델이고, 다른 하나는 텍스트 전용 입력에 최적화된 27B 변형 모델입니다. 두 모델 모두 텍스트를 출력합니다. MedGemma 4B는 Vision Question Answering (VQA) 벤치마크에서 이전의 SOTA 모델인 Med-Gemini (Saab et al., 2024; Yang et al., 2024)에 비해 훨씬 작은 규모임에도 불구하고 강력한 성능을 보여줍니다. MedGemma 4B와 27B 모두 MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), MMLU Med (Hendrycks et al., 2020), AfriMed-QA (Olatunji et al., 2024) 및 AgentClinic (Schmidgall et al., 2024)를 포함한 까다로운 텍스트 전용 의료 벤치마크 작업에서 비슷한 규모의 다른 오픈 모델과 비교했을 때 매우 경쟁력이 있습니다. 이러한 강력한 기본 기능 외에도, MedGemma를 흉부 X선 보고, 조직병리학 분류, 전자 건강 기록 정보 검색과 같은 하위 도메인에 대해 미세 조정함으로써 성능을 더욱 향상시킬 수 있는 방법을 보여줍니다.\n\n## 단어 목록 및 한글 의미:\n\n*   **Report:** 보고서\n*   **Focus:** 집중하다, 초점을 맞추다\n*   **MedGemma:** (모델 이름) MedGemma\n*   **Models:** 모델들\n*   **Variant:** 변형 (모델)\n*   **Accept:** 받아들이다, 수용하다\n*   **Text:** 텍스트, 글\n*   **Images:** 이미지, 그림\n*   **Input:** 입력\n*   **Optimized:** 최적화된\n*   **Text-only:** 텍스트 전용\n*   **Output:** 출력\n*   **Demonstrates:** 보여주다, 입증하다\n*   **Strong:** 강력한\n*   **Performance:** 성능\n*   **Vision Question Answering (VQA):** 시각 질의 응답\n*   **Benchmarks:** 벤치마크\n*   **Compared:** 비교했을 때\n*   **Prior:** 이전의, 우선하는\n*   **SOTA (State-of-the-art):** 최첨단 기술, 최고 수준\n*   **Considerably:** 상당히, 꽤\n*   **Smaller:** 더 작은\n*   **Highly:** 매우, 고도로\n*   **Competitive:** 경쟁력 있는\n*   **Challenging:** 도전적인, 어려운\n*   **Medical:** 의료의, 의학적인\n*   **Tasks:** 작업, 과제\n*   **Including:** ~을 포함하여\n*   **Open:** 공개된, 오픈 소스의\n*   **Similar:** 비슷한, 유사한\n*   **Scale:** 규모, 크기\n*   **In addition:** 게다가, 추가적으로\n*   **Capabilities:** 능력, 역량\n*   **Show:** 보여주다, 제시하다\n*   **Further:** 더, 더욱\n*   **Improved:** 개선된, 향상된\n*   **Fine-tuning:** 미세 조정\n*   **Subdomains:** 하위 도메인\n*   **Chest X-ray reporting:** 흉부 X선 보고\n*   **Histopathology:** 조직병리학\n*   **Classification:** 분류\n*   **Electronic health record:** 전자 건강 기록\n*   **Information retrieval:** 정보 검색\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: In this report, we focus on two MedGemma models: a 4B variant that can accept text, images,\nor both as input, and a 27B variant that is optimized for text-only inputs. Both models output text.\n### 번역: 본 보고서에서는 MedGemma 모델 두 가지에 초점을 맞춥니다. 하나는 텍스트, 이미지 또는 둘 다를 입력으로 받을 수 있는 4B 모델이고, 다른 하나는 텍스트 전용 입력에 최적화된 27B 모델입니다. 두 모델 모두 텍스트를 출력합니다.\n---\n\n\n### 원문: MedGemma 4B demonstrates strong performance on Vision Question Answering (VQA) benchmarks\ncompared to prior SOTA models like Med-Gemini (Saab et al., 2024; Yang et al., 2024) despite\nbeing considerably smaller.\n### 번역: MedGemma 4B는 훨씬 작은 크기에도 불구하고 Med-Gemini (Saab 외, 2024; Yang 외, 2024)와 같은 기존 최고 성능(SOTA) 모델과 비교하여 시각 질의 응답(VQA) 벤치마크에서 뛰어난 성능을 보여줍니다.\n---\n\n\n### 원문: Both MedGemma 4B and 27B are highly competitive on challenging\ntext-only medical benchmark tasks, including MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022),\nPubMedQA (Jin et al., 2019), MMLU Med (Hendrycks et al., 2020), AfriMed-QA (Olatunji et al.,\n2024), and AgentClinic (Schmidgall et al., 2024) when compared against other open models of\nsimilar scale.\n### 번역: MedGemma 4B 및 27B는 MedQA (Jin 외, 2021), MedMCQA (Pal 외, 2022), PubMedQA (Jin 외, 2019), MMLU Med (Hendrycks 외, 2020), AfriMed-QA (Olatunji 외, 2024) 및 AgentClinic (Schmidgall 외, 2024)을 포함하여 까다로운 텍스트 전용 의료 벤치마크 작업에서 비슷한 규모의 다른 오픈 모델과 비교했을 때 매우 경쟁력이 있습니다.\n---\n\n\n### 원문: In addition to these strong out-of-the-box capabilities, we show how performance can be\nfurther improved by fine-tuning MedGemma on subdomains like chest X-ray reporting, histopathology\nclassification, and electronic health record information retrieval.\n### 번역: 이러한 강력한 기본 기능 외에도, MedGemma를 흉부 X선 보고, 조직병리학 분류, 전자 건강 기록 정보 검색과 같은 하위 도메인에서 미세 조정하여 성능을 더욱 향상시킬 수 있는 방법을 보여줍니다.\n---\n\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n추가적으로, MedGemma 27B의 멀티모달 버전인 MedGemma 변형 모델이 개발되었으며, 다른 모델들과 함께 공개될 예정입니다. 이 멀티모달 27B 변형 모델에 대한 더 철저한 평가는 진행 중이며, 예비 결과는 부록 섹션 F에서 확인할 수 있습니다. 이 보고서에서 달리 언급되지 않는 한, \"MedGemma 27B\"를 언급하는 평가는 MedGemma 27B의 텍스트 전용 변형 모델을 의미합니다.\n\nMedGemma 모델 외에도, 독립형 MedSigLIP 4억 파라미터 의료 이미지 인코더에 대해 설명합니다. MedSigLIP은 SigLIP-400M (Zhai et al., 2023)을 기반으로 하며, MedGemma의 이미지 해석 능력을 지원하는 동일한 인코더입니다. 단독으로 사용될 때 MedSigLIP은 데이터 효율적이고 제로샷 이미지 분류 및 검색을 가능하게 하며, 성능은 특화된 이미지 인코더와 비슷하거나 능가합니다.\n\n공개된 모델에 대한 개략적인 개요는 그림 1에 나와 있습니다. MedGemma 컬렉션에 대한 더 자세한 정보 (튜토리얼 및 위의 모든 모델 다운로드 링크 포함)는 https://goo.gle/medgemma 에서 확인할 수 있습니다.\n\n## 단어 및 의미 리스트\n\n다음은 원문에서 추출한 단어들과 각각의 한글 의미입니다. 일부 단어는 문맥에 따라 다른 의미를 가질 수 있습니다.\n\n*   **additional:** 추가적인\n*   **MedGemma:** (모델 이름) 메드젬마\n*   **variant:** 변형, 이형\n*   **multimodal:** 멀티모달의, 다중 모드의\n*   **developed:** 개발된\n*   **released:** 공개된, 출시된\n*   **models:** 모델들\n*   **thorough:** 철저한\n*   **evaluation:** 평가\n*   **ongoing:** 진행 중인\n*   **preliminary:** 예비의, 초기의\n*   **results:** 결과\n*   **Appendix:** 부록\n*   **Section:** 섹션\n*   **unless:** ~하지 않는 한\n*   **otherwise:** 달리, 다른 방식으로\n*   **noted:** 언급된\n*   **report:** 보고서\n*   **reference:** 언급하다, 참조하다\n*   **text-only:** 텍스트 전용의\n*   **standalone:** 독립형의\n*   **MedSigLIP:** (모델 이름) 메드시그립\n*   **parameter:** 파라미터, 매개변수\n*   **medical:** 의료의\n*   **image:** 이미지, 영상\n*   **encoder:** 인코더\n*   **based on:** ~에 기반한\n*   **capabilities:** 능력, 역량\n*   **interpretation:** 해석\n*   **enables:** 가능하게 하다\n*   **data-efficient:** 데이터 효율적인\n*   **zero-shot:** 제로샷 (학습 없이 바로 적용하는 방식)\n*   **classification:** 분류\n*   **retrieval:** 검색\n*   **performance:** 성능\n*   **comparable:** 비교 가능한, 비슷한\n*   **exceeding:** 능가하는, 초과하는\n*   **specialized:** 특화된, 전문적인\n*   **high level overview:** 개략적인 개요\n*   **shown:** 보여지는, 나타난\n*   **details:** 세부 사항, 자세한 정보\n*   **collection:** 컬렉션, 모음\n*   **including:** 포함하여\n*   **tutorials:** 튜토리얼, 안내\n*   **links:** 링크\n*   **download:** 다운로드\n\n이 목록은 텍스트에 나타난 주요 단어들을 포함하고 있습니다. 더 자세한 의미를 알고 싶으시면 해당 단어를 검색해 보시는 것을 추천합니다.\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: An additional MedGemma variant, a multimodal version of MedGemma 27B, was also developed and is being released along with the other models.\n### 번역: 메드젬마 27B의 멀티모달 버전인 메드젬마 추가 변형 모델도 개발되어 다른 모델들과 함께 공개됩니다.\n---\n\n\n### 원문: More thorough evaluation of this multimodal 27B variant is ongoing and preliminary results can be found in Appendix Section F.\n### 번역: 이 멀티모달 27B 변형 모델에 대한 더 철저한 평가는 현재 진행 중이며, 예비 결과는 부록 섹션 F에서 확인할 수 있습니다.\n---\n\n\n### 원문: Unless otherwise noted in this report, evaluations that reference “MedGemma 27B” refer to the text-only variant of MedGemma 27B.\n### 번역: 이 보고서에서 달리 언급되지 않는 한, \"MedGemma 27B\"를 참조하는 평가는 MedGemma 27B의 텍스트 전용 변형 모델을 의미합니다.\n---\n\n\n### 원문: In addition to the MedGemma models, we describe the standalone MedSigLIP 400M-parameter medical image encoder.\n### 번역: 메드젬마 모델 외에도, 독립형 메드SigLIP 4억 파라미터 의료 이미지 인코더에 대해 설명합니다.\n---\n\n\n### 원문: MedSigLIP is based on SigLIP-400M (Zhai et al., 2023) and is the same encoder that powers MedGemma’s image interpretation capabilities.\n### 번역: 메드SigLIP은 SigLIP-400M (Zhai et al., 2023)을 기반으로 하며, 메드젬마의 이미지 해석 기능을 구동하는 것과 동일한 인코더입니다.\n---\n\n\n### 원문: When used on its own, MedSigLIP enables data-efficient and zero-shot image classification and retrieval, with performance comparable to or exceeding specialized image encoders.\n### 번역: 독립적으로 사용될 때, 메드SigLIP은 데이터 효율적이고 제로샷 이미지 분류 및 검색을 가능하게 하며, 성능은 전문 이미지 인코더와 비슷하거나 능가합니다.\n---\n\n\n### 원문: A high level overview of the released models is shown in Fig. 1.\n### 번역: 공개된 모델에 대한 개요는 그림 1에 나와 있습니다.\n---\n\n\n### 원문: More details about the MedGemma collection, including tutorials and links to download all of the above models, can be found at https://goo.gle/medgemma.\n### 번역: 튜토리얼과 위에 언급된 모든 모델을 다운로드할 수 있는 링크를 포함하여 MedGemma 컬렉션에 대한 자세한 내용은 https://goo.gle/medgemma 에서 확인할 수 있습니다.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역\n\n2. 방법론\n2.1. 데이터셋\n사전 학습 동안 일반적인 목적의 데이터 재현을 위해, SigLIP (Zhai et al., 2023) 및 Gemma 3 (Gemma-Team et al., 2025)의 원본 데이터 혼합을 활용했습니다. 의료 학습 및 평가 데이터셋은 Med-Gemini (Yang et al., 2024)의 데이터셋을 대체로 따랐습니다. 이 섹션에서는 Med-Gemini와 비교하여 데이터셋의 구체적인 변경 사항 또는 차이점을 설명합니다.\n\n## 단어 추출 및 한글 의미 리스트\n\n* **Methods:** 방법론\n* **Datasets:** 데이터셋\n* **General:** 일반적인, 보편적인\n* **Purpose:** 목적\n* **Data:** 데이터\n* **Replay:** 재현, 재생\n* **Pretraining:** 사전 학습\n* **Original:** 원본의, 원래의\n* **Mixtures:** 혼합, 혼합물\n* **Leveraged:** 활용했다, 이용했다\n* **Medical:** 의료의\n* **Training:** 학습\n* **Evaluation:** 평가\n* **Largely:** 대체로, 주로\n* **Followed:** 따랐다, 준수했다\n* **Section:** 섹션, 부분\n* **Outline:** 설명하다, 개요를 설명하다\n* **Specific:** 구체적인, 특정한\n* **Changes:** 변경 사항, 변화\n* **Differences:** 차이점\n* **Relative:** 상대적인, 비교하여\n\n**고유 명사 및 약어:**\n\n* **SigLIP:** (고유 명사, 논문 이름)\n* **Gemma 3:** (고유 명사, 모델 이름)\n* **Med-Gemini:** (고유 명사, 모델 이름)\n* **Zhai et al.:** (고유 명사, 저자 이름)\n* **Gemma-Team et al.:** (고유 명사, 저자 그룹 이름)\n* **Yang et al.:** (고유 명사, 저자 이름)\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 2. Methods\n### 번역: 2. 방법론\n---\n\n\n### 원문: 2.1. Datasets\n### 번역: 2.1. 데이터셋\n---\n\n\n### 원문: For general purpose data replay during pretraining, original data mixtures from SigLIP (Zhai et al.,\n2023) and Gemma 3 (Gemma-Team et al., 2025) were leveraged.\n### 번역: 사전 훈련 과정에서 일반적인 목적의 데이터 재학습을 위해, SigLIP (Zhai et al., 2023)과 Gemma 3 (Gemma-Team et al., 2025)의 원본 데이터 혼합을 활용했습니다.\n---\n\n\n### 원문: The medical training and evaluation\ndatasets largely followed the datasets in Med-Gemini (Yang et al., 2024).\n### 번역: 의료 학습 및 평가 데이터셋은 Med-Gemini (Yang et al., 2024)의 데이터셋을 대체로 따랐습니다.\n---\n\n\n### 원문: In this section, we outline\nthe specific changes or differences in datasets relative to Med-Gemini.\n### 번역: 이 섹션에서는 Med-Gemini와 비교하여 데이터셋에 나타난 구체적인 변경 사항이나 차이점을 설명합니다.\n---\n\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n2.1.1. 훈련 데이터셋\n\n텍스트 전용 데이터셋: 텍스트 데이터셋의 경우, 우리는 MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), MedExpQA (Alonso et al., 2024), AfriMed-QA (Olatunji et al., 2024), HealthSearchQA (Singhal et al., 2023a), 그리고 LiveQA (Abacha et al., 2017)를 포함한 여러 의료 QA 데이터셋의 훈련 분할을 사용하여 대규모 IT(instruction-tuned, 명령어 튜닝된) 교사로부터 응답과 로짓을 샘플링했습니다. 우리는 또한 위 데이터셋에서 무작위로 샘플링된 5개의 질문을 예시로 사용하여 동일한 대규모 IT 교사에게 새로운 질문을 생성하도록 요청하여 생성된 약 20만 개의 합성 의료 질문에 대한 응답과 로짓을 샘플링했습니다.\n\n## 단어 목록 및 의미:\n\n*   **Training:** 훈련\n*   **datasets:** 데이터셋\n*   **Text-only:** 텍스트 전용\n*   **For:** ~을 위해, ~에 대하여\n*   **text:** 텍스트\n*   **we:** 우리\n*   **sampled:** 샘플링했다, 표본으로 추출했다\n*   **responses:** 응답\n*   **logits:** 로짓 (확률을 나타내는 수학적 함수)\n*   **from:** ~로부터\n*   **large:** 큰, 대규모의\n*   **IT (instruction-tuned):** IT (instruction-tuned, 명령어 튜닝된)\n*   **teacher:** 교사 (여기서는 AI 모델을 의미)\n*   **using:** 사용해서\n*   **train:** 훈련\n*   **splits:** 분할\n*   **of:** ~의\n*   **multiple:** 여러, 다수의\n*   **medical:** 의료\n*   **QA:** QA (Question Answering, 질의 응답)\n*   **datasets:** 데이터셋\n*   **including:** ~을 포함하여\n*   **MedQA:** MedQA (데이터셋 이름)\n*   **MedMCQA:** MedMCQA (데이터셋 이름)\n*   **PubMedQA:** PubMedQA (데이터셋 이름)\n*   **MedExpQA:** MedExpQA (데이터셋 이름)\n*   **AfriMed-QA:** AfriMed-QA (데이터셋 이름)\n*   **HealthSearchQA:** HealthSearchQA (데이터셋 이름)\n*   **LiveQA:** LiveQA (데이터셋 이름)\n*   **also:** 또한\n*   **approximately:** 대략, 약\n*   **synthetic:** 합성의, 인공적인\n*   **questions:** 질문\n*   **generated:** 생성된\n*   **by:** ~에 의해\n*   **asking:** 요청하여\n*   **same:** 동일한\n*   **to:** ~에게, ~하기 위해\n*   **generate:** 생성하다\n*   **new:** 새로운\n*   **question:** 질문\n*   **randomly:** 무작위로\n*   **sampled:** 샘플링된, 추출된\n*   **questions:** 질문\n*   **as:** ~로서\n*   **examples:** 예시\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 2.1.1. Training datasets\n### 번역: 2.1.1. 학습 데이터 세트\n---\n\n\n### 원문: Text-only datasets: For text datasets, we sampled responses and logits from a large IT (instructiontuned)\nteacher using the train splits of multiple medical QA datasets, including MedQA (Jin et al.,\n2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), MedExpQA (Alonso et al., 2024),\nAfriMed-QA (Olatunji et al., 2024), HealthSearchQA (Singhal et al., 2023a), and LiveQA (Abacha\net al., 2017).\n### 번역: 텍스트 전용 데이터 세트: 텍스트 데이터 세트를 위해, 저희는 MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), MedExpQA (Alonso et al., 2024), AfriMed-QA (Olatunji et al., 2024), HealthSearchQA (Singhal et al., 2023a), 그리고 LiveQA (Abacha et al., 2017)를 포함한 여러 의료 QA 데이터 세트의 학습 분할에서 대규모 IT (명령어 튜닝) 교사를 사용하여 응답과 로짓을 샘플링했습니다.\n---\n\n\n### 원문: We also sampled responses and logits for approximately 200,000 synthetic medical\nquestions generated by asking the same large IT teacher to generate a new question using 5 randomly\nsampled questions from the above datasets as examples.\n### 번역: 또한, 위 데이터 세트에서 무작위로 샘플링된 5개의 질문을 예시로 사용하여 새로운 질문을 생성하도록 동일한 대규모 IT 교사에게 요청하여 생성된 약 20만 개의 합성 의료 질문에 대한 응답과 로짓을 샘플링했습니다.\n---\n\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n**다중 모드 데이터셋:** Med-Gemini와 비교했을 때, MedGemma의 다중 모드 능력은 현재 2D 의료 이미지 (예: X-ray, CT/MRI의 2D 슬라이스)에 집중되어 있습니다. Yang et al. (2024)에 설명된 3D 볼륨 및 유전체 데이터셋은 포함되지 않았습니다. 또한, 저희와 다른 연구자들은 PathVQA 및 MedVQA에서 잠재적인 데이터 품질 문제를 확인했습니다. 따라서 저희는 해당 데이터셋들을 훈련 데이터셋에서 제거했습니다. PAD-UFES-20은 매우 특정 병변 유형의 6-클래스 분류에 초점을 맞추고 있어, 더 일반적인 목적의 피부과 능력 및 사용 사례의 목표와 일치하지 않으므로 사후 훈련 데이터셋에 포함하지 않았습니다. 훈련 데이터의 PMC-OA 구성 요소의 경우, 더 나은 데이터 품질을 위해 PMC-OA에서 단일 패널 의료 이미지만 포함했습니다. Med-Gemini와 비교하여 안과 (184,852개의 더 많은 망막 기저 사진), 피부과 (210가지 다른 피부 질환을 가진 51,049개의 더 많은 피부과 이미지), 조직 병리학 (총 ∼3250만 개의 패치-텍스트 쌍), 그리고 방사선학 데이터 (54,573개의 더 많은 CT 2D 슬라이스, 47,622개의 더 많은 MRI 2D 슬라이스)를 위한 더 큰 내부 컬렉션을 도입했습니다. 훈련에 활용된 추가 CT 및 MRI 슬라이스는 방사선 보고서에서 비정상 소견과 관련된 특정 슬라이스 언급을 기반으로 큐레이션되었습니다.\n\n## 단어 추출 및 한글 의미 리스트:\n\n*   **Multimodal:** 다중 모드\n*   **Datasets:** 데이터셋\n*   **Relative:** 상대적인, 비교하여\n*   **Med-Gemini:** (이름) Med-Gemini\n*   **MedGemma:** (이름) MedGemma\n*   **Capabilities:** 능력\n*   **Currently:** 현재\n*   **Focused:** 집중된\n*   **2D:** 2차원\n*   **Medical:** 의료의\n*   **Images:** 이미지\n*   **e.g.:** 예를 들어\n*   **X-ray:** 엑스레이\n*   **Slices:** 슬라이스\n*   **CT:** (컴퓨터 단층 촬영) CT\n*   **MRI:** (자기 공명 영상) MRI\n*   **3D:** 3차원\n*   **Volumes:** 볼륨\n*   **Genomic:** 유전체의\n*   **Described:** 설명된\n*   **Included:** 포함된\n*   **Additionally:** 추가적으로\n*   **Identified:** 확인된\n*   **Potential:** 잠재적인\n*   **Data:** 데이터\n*   **Quality:** 품질\n*   **Issues:** 문제\n*   **PathVQA:** (이름) PathVQA\n*   **MedVQA:** (이름) MedVQA\n*   **Thus:** 따라서\n*   **Removed:** 제거된\n*   **Training:** 훈련\n*   **Dataset:** 데이터셋\n*   **PAD-UFES-20:** (이름) PAD-UFES-20\n*   **Post-training:** 사후 훈련\n*   **Since:** 때문에\n*   **Focuses:** 초점을 맞추다\n*   **Classification:** 분류\n*   **Lesion:** 병변\n*   **Types:** 유형\n*   **Line:** 일치\n*   **Goal:** 목표\n*   **General:** 일반적인\n*   **Purpose:** 목적\n*   **Dermatology:** 피부과\n*   **Use Cases:** 사용 사례\n*   **Component:** 구성 요소\n*   **PMC-OA:** (이름) PMC-OA\n*   **Single:** 단일의\n*   **Panel:** 패널\n*   **Better:** 더 나은\n*   **Collection:** 컬렉션\n*   **Ophthalmology:** 안과\n*   **Retinal:** 망막의\n*   **Fundus:** 기저\n*   **Histopathology:** 조직 병리학\n*   **Total:** 총\n*   **Patch-text pairs:** 패치-텍스트 쌍\n*   **Radiology:** 방사선학\n*   **Additional:** 추가적인\n*   **Utilized:** 활용된\n*   **Curated:** 큐레이션된\n*   **Based on:** 기반으로\n*   **Mention:** 언급\n*   **Specific:** 특정\n*   **Associated:** 관련된\n*   **Abnormal:** 비정상적인\n*   **Findings:** 소견\n*   **Report:** 보고서\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: Multimodal datasets: Relative to Med-Gemini, the multimodal capabilities of MedGemma are currently focused on 2D medical images (e.g. X-ray, 2D slices from CT/MRI); 3D volumes and genomic datasets described in Yang et al. (2024) were not included.\n### 번역: 멀티모달 데이터셋: Med-Gemini와 비교했을 때, MedGemma의 멀티모달 기능은 현재 2D 의료 이미지(예: X-ray, CT/MRI의 2D 슬라이스)에 초점을 맞추고 있으며, Yang et al. (2024)에서 설명된 3D 볼륨 및 유전체 데이터셋은 포함되지 않았습니다.\n---\n\n\n### 원문: Additionally, we and others have identified potential data quality issues in PathVQA and MedVQA. Thus, we removed them from the training dataset.\n### 번역: 추가적으로, 저희와 다른 연구진은 PathVQA와 MedVQA에서 잠재적인 데이터 품질 문제를 확인했습니다. 따라서, 저희는 해당 데이터들을 훈련 데이터셋에서 제거했습니다.\n---\n\n\n### 원문: We did not include PAD-UFES-20 in the post-training dataset since it focuses on 6-class classification of very specific lesion types, which is not in line with the goal of more general purpose dermatology capabilities and use cases.\n### 번역: PAD-UFES-20은 매우 특정한 병변 유형의 6가지 클래스 분류에 초점을 맞추고 있어, 더 일반적인 목적의 피부과 기능 및 활용 사례라는 목표와 일치하지 않으므로, 사후 훈련 데이터셋에 포함하지 않았습니다.\n---\n\n\n### 원문: For the PMC-OA component of the training data, we only included the single panel medical images from PMC-OA for better data quality.\n### 번역: 훈련 데이터의 PMC-OA 구성 요소에 대해서는, 더 나은 데이터 품질을 위해 PMC-OA의 단일 패널 의료 이미지만 포함했습니다.\n---\n\n\n### 원문: Relative to Med-Gemini we also introduced a larger internal collection for ophthalmology (184,852 more retinal fundus images), dermatology (51,049 more dermatology images with 210 different skin conditions), histopathology (a total of ∼32.5 million patch-text pairs), and radiology data (54,573 more CT 2D slices, 47,622 more MRI 2D slices).\n### 번역: Med-Gemini와 비교했을 때, 저희는 또한 안과(망막저 사진 184,852장 추가), 피부과(210가지 다른 피부 질환이 있는 피부과 이미지 51,049장 추가), 조직병리학(총 약 3,250만 개의 패치-텍스트 쌍), 그리고 방사선학 데이터(CT 2D 슬라이스 54,573장 추가, MRI 2D 슬라이스 47,622장 추가)에 대한 더 큰 내부 데이터 수집을 도입했습니다.\n---\n\n\n### 원문: The additional CT and MRI slices utilized for training were curated based on mention of a specific slice associated with abnormal findings in the radiology report.\n### 번역: 훈련에 사용된 추가 CT 및 MRI 슬라이스는 방사선학 보고서에서 이상 소견과 관련된 특정 슬라이스의 언급을 기반으로 선별되었습니다.\n---\n\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n2.1.2. 데이터 전처리\n우리의 데이터 준비 과정은 Yang et al. (2024)을 면밀히 따랐습니다. 이미지 패딩 및 리사이징 알고리즘은 동일하게 유지되었지만, Gemma 3에서는 비전 인코더가 다르기 때문에 이미지를 768×768 대신 896×896으로 리사이징했습니다. Gemma 3에 따라, 262,000개의 항목을 가진 SentencePiece 토크나이저를 사용합니다. 추가적으로, CT 이미지의 경우, 세 개의 윈도우를 미리 선택하여 입력 이미지의 RGB 색상 채널로 변환하여 다음을 강조했습니다: (1) 뼈와 폐, 윈도우 폭: 2250, 윈도우 레벨: -100; (2) 연조직, 윈도우 폭: 350, 윈도우 레벨: 40; (3) 뇌, 윈도우 폭: 80, 윈도우 레벨: 40.\n\n## 단어 추출 및 의미:\n\n*   **Data:** 데이터\n*   **Preprocessing:** 전처리\n*   **Preparation:** 준비\n*   **Followed:** 따랐습니다 (따르다)\n*   **Closely:** 면밀히, 자세히\n*   **Image:** 이미지\n*   **Padding:** 패딩 (이미지 주변에 빈 공간을 추가하는 기술)\n*   **Resizing:** 리사이징 (크기 조정)\n*   **Algorithms:** 알고리즘\n*   **Remain:** 유지되다\n*   **Same:** 동일한\n*   **Vision:** 비전 (시각)\n*   **Encoder:** 인코더 (정보를 암호화하는 장치/프로그램)\n*   **Different:** 다른\n*   **Instead of:** ~ 대신에\n*   **Tokenizer:** 토크나이저 (텍스트를 토큰으로 분리하는 도구)\n*   **Entries:** 항목\n*   **Additionally:** 추가적으로\n*   **CT images:** CT 이미지 (컴퓨터 단층 촬영 이미지)\n*   **Preselected:** 미리 선택된\n*   **Windows:** 윈도우 (특정 범위의 값)\n*   **Converted:** 변환된 (변환되다)\n*   **RGB:** RGB (Red, Green, Blue, 빨강, 초록, 파랑)\n*   **Color:** 색상\n*   **Channels:** 채널\n*   **Input:** 입력\n*   **Highlight:** 강조하다\n*   **Bone:** 뼈\n*   **Lung:** 폐\n*   **Window-width:** 윈도우 폭\n*   **Window-level:** 윈도우 레벨\n*   **Soft tissue:** 연조직\n*   **Brain:** 뇌\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 2.1.2. Data Preprocessing\n### 번역: 2.1.2. 데이터 전처리\n---\n\n\n### 원문: Our data preparation followed Yang et al. (2024) closely.\n### 번역: 우리의 데이터 준비 과정은 Yang 외 (2024)의 방법을 거의 그대로 따랐습니다.\n---\n\n\n### 원문: Image padding and resizing algorithms remain the same, but because the vision encoder is different in Gemma 3, our images were resized to 896×896 instead of 768×768.\n### 번역: 이미지 패딩 및 리사이징 알고리즘은 동일하게 유지했지만, Gemma 3에서는 비전 인코더가 다르기 때문에 이미지 크기를 768x768 대신 896x896으로 조정했습니다.\n---\n\n\n### 원문: Following Gemma 3, we use the SentencePiece tokenizer with 262,000 entries.\n### 번역: Gemma 3을 따라, 우리는 262,000개의 항목을 가진 SentencePiece 토크나이저를 사용합니다.\n---\n\n\n### 원문: Additionally, for CT images, we preselected three windows and converted them into the RGB color channels of the input image to highlight (1) bone and lung, window-width: 2250, window-level: -100; (2) soft tissue, window-width: 350, window-level: 40; (3) brain, window-width: 80, window-level: 40.\n### 번역: 추가적으로, CT 이미지의 경우, 우리는 세 개의 윈도우를 미리 선택하여 입력 이미지의 RGB 색상 채널로 변환하여 (1) 뼈와 폐 (윈도우 폭: 2250, 윈도우 레벨: -100), (2) 연조직 (윈도우 폭: 350, 윈도우 레벨: 40), (3) 뇌 (윈도우 폭: 80, 윈도우 레벨: 40)을 강조했습니다.\n"
    },
    {
      "type": "한글→영어 (단어 분석) [gemini-2.0-flash]",
      "content": "## 한국어 번역:\n\n2.2. 모델링 방법론\n2.2.1. 모델링 구조 및 학습 인프라\nMedGemma 모델 구조는 Gemma 3 (Gemma-Team 외., 2025)을 따르며, 기존의 모든 Gemma 인프라와 호환됩니다. Gemma 3의 시각 인코더는 SigLIP 인코더의 400M 변형 (Zhai 외., 2023)이며, 다양한 Gemma 언어 모델 크기 (4B, 27B)에서 공유됩니다. 입력 이미지 해상도는 896×896이며, 픽셀 값은 [-1, 1]로 정규화됩니다. 언어 모델 구성 요소 또한 Gemma 3을 따르며, 임의의 이미지-텍스트 인터리빙과 긴 컨텍스트 (128k)를 특징으로 합니다. Gemma 3과 유사하게 MedGemma는 TPUv4, TPUv5e, TPUv5p에서 학습되었으며, 메모리 절약을 위해 미리 계산된 시각적 토큰을 활용하고, 멀티-팟 학습을 위해 데이터 및 모델 샤딩을 사용했습니다.\n\n## 단어 추출 및 한글 의미:\n\n*   **Modeling:** 모델링\n*   **Methodology:** 방법론\n*   **Architecture:** 구조\n*   **Training:** 학습\n*   **Infrastructure:** 인프라\n*   **Model:** 모델\n*   **Compatible:** 호환되는\n*   **Existing:** 기존의, 존재하는\n*   **Vision:** 시각\n*   **Encoder:** 인코더\n*   **Variant:** 변형, 변종\n*   **Language:** 언어\n*   **Sizes:** 크기\n*   **Input:** 입력\n*   **Image:** 이미지\n*   **Resolution:** 해상도\n*   **Pixel:** 픽셀\n*   **Values:** 값\n*   **Normalized:** 정규화된\n*   **Component:** 구성 요소, 요소\n*   **Featuring:** 특징으로 하는\n*   **Arbitrary:** 임의의\n*   **Interleaving:** 인터리빙, 교차 배치\n*   **Context:** 컨텍스트, 문맥, 맥락\n*   **Similar:** 유사한\n*   **Trained:** 학습된\n*   **Leveraged:** 활용된, 이용된\n*   **Pre-computed:** 미리 계산된\n*   **Visual:** 시각적인\n*   **Tokens:** 토큰, 기호\n*   **Memory:** 메모리, 기억 장치\n*   **Saving:** 절약\n*   **Used:** 사용된\n*   **Data:** 데이터\n*   **Shardings:** 샤딩 (데이터베이스 분할)\n*   **Multi-pod:** 멀티 팟\n"
    },
    {
      "type": "한국어 번역 [gemini-2.0-flash]",
      "content": "---\n\n\n### 원문: 2.2. Modeling Methodology\n### 번역: 2.2. 모델링 방법론\n---\n\n\n### 원문: 2.2.1. Modeling Architecture and Training Infrastructure\n### 번역: 2.2.1. 모델링 아키텍처 및 학습 인프라\n---\n\n\n### 원문: The MedGemma model architecture follows Gemma 3 (Gemma-Team et al., 2025) and is compatible\nwith all existing Gemma infrastructure.\n### 번역: MedGemma 모델 아키텍처는 Gemma 3 (Gemma-Team 외, 2025)을 따르며, 기존의 모든 Gemma 인프라와 호환됩니다.\n---\n\n\n### 원문: The vision encoder for Gemma 3 is the 400M variant of\nthe SigLIP encoder (Zhai et al., 2023) and is shared across the different Gemma language model\nsizes (4B, 27B).\n### 번역: Gemma 3의 비전 인코더는 SigLIP 인코더 (Zhai 외, 2023)의 4억 파라미터 변형이며, 서로 다른 크기의 Gemma 언어 모델 (4B, 27B)에서 공유됩니다.\n---\n\n\n### 원문: The input image resolution is 896×896 with pixel values normalized to [-1, 1].\n### 번역: 입력 이미지 해상도는 896x896이며, 픽셀 값은 [-1, 1]로 정규화됩니다.\n---\n\n\n### 원문: The\nlanguage model component also follows Gemma 3, featuring arbitrary image-text interleaving and\nlong context (128k).\n### 번역: 언어 모델 구성 요소 또한 Gemma 3을 따르며, 임의의 이미지-텍스트 인터리빙과 긴 문맥 (128k)을 특징으로 합니다.\n---\n\n\n### 원문: Similar to Gemma 3, MedGemma was trained on TPUv4, TPUv5e, and TPUv5p,\nleveraged pre-computed visual tokens for memory saving, and used data and model shardings for\nmulti-pod training.\n### 번역: Gemma 3과 유사하게, MedGemma는 TPUv4, TPUv5e, TPUv5p에서 학습되었으며, 메모리 절약을 위해 미리 계산된 시각적 토큰을 활용하고, 멀티팟 학습을 위해 데이터 및 모델 샤딩을 사용했습니다.\n---\n\n\n### 원문: 4\n### 번역: 4\n---\n\n\n"
    }
  ]
}